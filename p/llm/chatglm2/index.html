<!DOCTYPE html>
<html lang="zh-cmn-Hans">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta http-equiv="X-UA-Compatible" content="ie=edge" />
        <title>ChatGLM2-6B - xiaodu114.github.io</title>
        <meta name="keywords" content="chatglm2-6b,chatglm2-6b-int4,å¤§è¯­è¨€æ¨¡å‹,llm,é‡åŒ–,CPU,æœ¬åœ°éƒ¨ç½²,fastapi,uvicorn,æµå¼å“åº”,å¾®è°ƒ" />
        <meta name="description" content="å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ - ChatGLM2-6B é‡åŒ–ç‰ˆ chatglm2-6b-int4 çš„æœ¬åœ°éƒ¨ç½²ï¼šwin10 + cpu" />

        <script src="/p/_/js/main.js"></script>
        <script src="../_common/main.js"></script>
    </head>

    <body>
        <!-- githubè®¿é—®åœ°å€ï¼š/p/llm/chatglm2/index.html -->
        <div class="blog-page">
            <h1>ChatGLM2-6B</h1>
            <p><line-code>ChatGLM2-6B</line-code>è¿˜æ˜¯å¾ˆå‰å®³çš„ã€‚å›½å†…çš„ï¼Œä¸­æ–‡æ”¯æŒæ¯”è¾ƒå‹å¥½ï¼Œå¿…é¡»å¾—æ”¯æŒä¸€ä¸‹ã€‚å¤šçš„å°±ä¸è¯´äº†ï¼Œè‡ªå·±çœ‹å®˜ç½‘æˆ–è€…æœæœå§ï¼è¿™é‡Œä¸»è¦è¯´ä¸€ä¸‹<line-code>ChatGLM2-6B</line-code>çš„æœ¬åœ°éƒ¨ç½²ã€‚å¯¹ï¼Œæ˜¯æœ¬åœ°éƒ¨ç½²å•Šã€‚é‡ç‚¹åœ¨è¯´ä¸€ä¸‹ï¼Œç”¨çš„æ˜¯CPUï¼Œç”¨çš„æ˜¯CPUï¼Œç”¨çš„æ˜¯CPUã€‚</p>
            <p>
                <a href="https://github.com/THUDM/ChatGLM2-6B" target="_blank">GitHub - THUDM/ChatGLM2-6B: ChatGLM2-6B: An Open Bilingual Chat LLM | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹</a>
            </p>
            <h2>æˆ‘çš„è€æœ‹å‹</h2>
            <p>éƒ½æ˜¯æ³ªå•Šï¼ç¡¬ä»¶è·Ÿä¸ä¸Šï¼Œåˆä¸Šä¸äº†äº‘ï¼Œåˆæƒ³æä¸€ä¸‹ï¼Œåªèƒ½å…ˆå¼„ä¸ªä¸ç‰ˆäº†â€¦â€¦å…ˆé€éœ²ä¸€ä¸‹ï¼Œé‚£å“åº”é€Ÿåº¦ç®€ç›´äº†â€¦â€¦</p>
            <mark-block explain="èº«ä½“æ¡ä»¶">
                <div style="display: flex">
                    <div style="flex: 1">
                        <p>OS åç§°ï¼š Microsoft Windows 10 æ•™è‚²ç‰ˆ</p>
                        <p>å¤„ç†å™¨ï¼š i5-4590T CPU @ 2.00GHz</p>
                        <p>ç³»ç»Ÿç±»å‹ï¼š x64-based PC</p>
                    </div>
                    <div style="flex: 1">
                        <p>OS ç‰ˆæœ¬ï¼š 10.0.19042 æš‚ç¼º Build 19042</p>
                        <p>æœºå¸¦RAMï¼š 12.0GB</p>
                        <p>GPUï¼š ä½ çŒœ</p>
                    </div>
                </div>
            </mark-block>
            <p>ä½ å¯ä»¥å‘½ä»¤è¡Œè¾“å…¥<line-code>systeminfo</line-code>æˆ–è€…<line-code>win + R</line-code>dxdiag æŸ¥çœ‹è¿™äº›ä¿¡æ¯ã€‚</p>
            <h2>è¿˜éœ€è¦ä»€ä¹ˆ</h2>
            <p>å…‰æœ‰ç¡¬ä»¶è¿˜ä¸è¡Œï¼Œè¿˜å¾—æ¥ç‚¹è½¯ä»¶ï¼Œå’±å¾—ä¸¤æ‰‹æŠ“ã€‚ç°åœ¨è¿™ä¸ªè¡Œæƒ…ï¼Œå¿…é¡»å¾—æŠŠ<line-code>Python</line-code>å¼„ä¸Šï¼Œéº»åˆ©ç‚¹ã€‚ä¸è¿‡ï¼Œå’±ä¹Ÿä¸èƒ½éª„å‚²å•Šï¼Œå¬è¯´<line-code>Rust</line-code>ã€<line-code>Mojo</line-code>å·²ç»è¡ŒåŠ¨äº†ã€‚å°¤å…¶æ˜¯è¿™ä¸ª<line-code>Rust</line-code>ï¼Œè…èš€æ€§å¤ªå¼ºäº†ï¼Œåæ›°ï¼šâ€œé”ˆåŒ–â€ï¼Œä¸æ˜¯å·²ç»é”ˆåŒ–ï¼Œå°±æ˜¯åœ¨è¢«é”ˆåŒ–çš„è·¯ä¸Šï¼Œå¤§æœ‰è…èš€ä¸€åˆ‡ä¹‹åŠ¿å•Šï¼›è¿™ä¸ª<line-code>Mojo</line-code>ä¹Ÿä¸ç®€å•å•Šï¼Œ35000è¿™ä¸ªæ•°å­—å°±ååˆ†å“äººã€‚æ‰€ä»¥å•Šï¼Œä½ å¾—åŠªåŠ›äº†<line-code>Python</line-code>ã€‚æŒ‰ç…§å®˜ç½‘çš„è¯´æ³•ï¼Œè¦æƒ³åœ¨CPUä¸Šè·‘ï¼Œè¿˜å¾—éœ€è¦GCCï¼Œé‚£å’±å°±å¾—å®‰æ’ä¸Šå•Šï¼Œæ˜¯å§ã€‚</p>
            <h3>Python</h3>
            <p>äººç”Ÿè‹¦çŸ­ï¼Œæˆ‘ç”¨Pythonã€‚ä¹‹å‰å·²ç»å®‰è£…è¿‡äº†ï¼Œç‰ˆæœ¬æ˜¯ï¼šPython 3.10.2ã€‚å¦‚æœæ²¡æœ‰å®‰è£…è¿‡ï¼Œå…ˆå»å®˜ç½‘ä¸‹è½½ä¸€ä¸‹ï¼š<a href="https://www.python.org/" target="_blank">Welcome to Python.org</a></p>
            <p>å¤šè¯´ä¸€å¥ï¼šå®‰è£…çš„æ—¶å€™ï¼Œè®°å¾—æ·»åŠ åˆ°<line-code>Path</line-code>ï¼Œä¹Ÿå°±æ˜¯æ·»åŠ åˆ°ç¯å¢ƒå˜é‡ï¼Œæ·»åŠ ä¹‹åä½¿ç”¨èµ·æ¥å°±æ¯”è¾ƒæ–¹ä¾¿äº†ã€‚</p>
            <p>æ³¨æ„ï¼šè¿™é‡Œè¿˜è®¾ç½®äº†<line-code>pip</line-code>çš„å…¨å±€é•œåƒæºï¼Œä½¿ç”¨çš„æ˜¯æ¸…åçš„ï¼š<a href="https://mirrors.tuna.tsinghua.edu.cn/help/pypi/" target="_blank"> pypi | é•œåƒç«™ä½¿ç”¨å¸®åŠ© | æ¸…åå¤§å­¦å¼€æºè½¯ä»¶é•œåƒç«™ | Tsinghua Open Source Mirror</a></p>
            <h3>GCC</h3>
            <p><line-code>ChatGLM2-6B</line-code>çš„GitHubæ–‡æ¡£ä»‹ç»ï¼Œè¿˜éœ€è¦GCCç¯å¢ƒï¼Œæ¨èçš„æ˜¯ï¼šTDM-GCCã€‚<a href="https://jmeubank.github.io/tdm-gcc/" target="_blank">tdm-gcc</a>ã€‚å®‰è£…çš„æ—¶å€™åˆ«å¿˜äº†å‹¾é€‰<line-code>openmp</line-code>ã€‚è¿™é‡Œçš„ç‰ˆæœ¬æ˜¯ï¼š10.3.0</p>
            <h2>å¥½æˆå¼€åœºäº†</h2>
            <h3>ä¸‹è½½æ¨¡å‹</h3>
            <p>åˆç«äº†ä¸€ä¸ªç«™ç‚¹ï¼šHugging Faceã€‚çœ‹çœ‹è¿™å‰ç¥¥ç‰©å¤šå¯çˆ±ï¼šğŸ¤—ã€‚æŒ‰ç†è¯´å’±åº”è¯¥å»äººå®¶æ‹¥æŠ±è„¸é‚£é‡Œä¸‹è½½ï¼Œéƒ½ä¸çŸ¥é“ç½‘ä¸Šå¥½å¤šæ•™ç¨‹çš„å¤§ç¥ä»¬æ˜¯æ€ä¹ˆä¸²é—¨çš„ï¼Œåæ­£æˆ‘è¿™é‡Œæ˜¯ä¸è¡Œå•Šï¼Œéš¾é“å¥¹æ²¡æœ‰ç»™æˆ‘å‘è¯·å¸–ï¼Œå¤ªéš¾äº†ã€‚è¿˜å¥½ï¼Œåˆç»™æˆ‘å¼€äº†ä¸€æ‰‡çª—ï¼š<a href="https://aliendao.cn/" target="_blank">äº’é“¾é«˜ç§‘</a>ï¼Œä¸è¿‡è¿™çª—æˆ·å§‹ç»ˆæ˜¯çª—æˆ·å•Šï¼Œå¤ªæ…¢äº†ã€‚è¿™é‡Œä¸å¾—ä¸è¯´ä¸€ä¸‹äººå®¶<line-code>ChatGLM2-6B</line-code>äº†ï¼Œå¤ªè´´å¿ƒäº†ï¼Œå°±ä¼šç»™å’±ä»¬è‡ªå·±äººå¼€å°å·®ï¼š<a href="https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/" target="_blank">æ¸…åå¤§å­¦äº‘ç›˜</a>ï¼Œä½ åœ¨è¿™é‡Œä¸‹è½½å°±çˆ½æ­ªæ­ªäº†ã€‚</p>
            <p>é‡ç‚¹ï¼šã€äº’é“¾é«˜ç§‘ã€‘ä¸‹è½½ã€æ¸…åå¤§å­¦äº‘ç›˜ã€‘ä¸­æ²¡æœ‰çš„æ–‡ä»¶ï¼Œä¸¤è€…åˆå¹¶ï¼Œå®Œç¾ã€‚</p>
            <p>è¿™é‡Œå°†ä¸‹è½½çš„æ¨¡å‹æ”¾åˆ°<line-code>D:\llm\THUDM\chatglm2-6b-int4</line-code>è·¯å¾„ä¸‹äº†ï¼Œè®°ä½è¿™ä¸ªè·¯å¾„ï¼Œä¸‹é¢ä¼šç”¨åˆ°ã€‚å¦‚ä¸‹å›¾ï¼š</p>
            <p>
                <img src="./image/1.png" alt="ChatGLM2-6B å¤§è¯­è¨€æ¨¡å‹ ç»“æ„ç›®å½•" />
            </p>
            <h3>ä¸‹è½½ä»“åº“</h3>
            <p>è¿™ä¸ªå°±æ¯”è¾ƒç®€å•äº†ï¼Œç›´æ¥å»GitHubæå°±è¡Œäº†ã€‚ä½ å¯ä»¥ä½¿ç”¨ git å…‹éš†ï¼›ä¹Ÿå¯ä»¥å®˜ç½‘ä¸Šä¸‹è½½zipå‹ç¼©åŒ…ï¼›å¦‚æœæœ‰å‘å¸ƒç‰ˆæœ¬ï¼Œä½ ä¹Ÿå¯ä»¥ä¸‹è½½ç¨³å®šçš„ç‰ˆæœ¬ï¼ˆè¿™é‡Œæ²¡æœ‰å•Šï¼‰ã€‚</p>
            <p>è¿™é‡Œå°†ä¸‹è½½çš„ä»“åº“æ”¾åˆ°<line-code>E:\llm\ChatGLM2-6B-main</line-code>è·¯å¾„ä¸‹äº†ï¼Œå¦‚ä¸‹å›¾ï¼š</p>
            <p>
                <img src="./image/2.png" alt="ChatGLM2-6B GitHubé¡¹ç›®ç»“æ„ç›®å½•" />
            </p>
            <p>è¯´æ˜ï¼šè¿™é‡Œçš„ä»“åº“æ˜¯ä»Šå¤©ï¼ˆ2023-10-16ï¼‰åœ¨GitHubè·å–çš„ã€‚</p>
            <h4>æ–°å»º modelPath.py æ–‡ä»¶</h4>
            <p>åœ¨ä»£ç ä¸­æœç´¢<line-code>THUDM/chatglm2-6b</line-code>ï¼Œç»“æœå‡ºç°äº†å¥½å¤šæ¬¡ï¼Œè¦ä¿®æ”¹æ—¶åŒæ­¥çš„åœ°æ–¹å¤ªå¤šäº†ï¼Œè¿™é‡Œæ–°å»ºçš„è¿™ä¸ªæ–‡ä»¶å°±æ˜¯è®¾ç½®å¤§æ¨¡å‹çš„ç»å¯¹è·¯å¾„ï¼Œç”¨çš„åœ°æ–¹ç›´æ¥å¼•å…¥å³å¯ã€‚è¯¥æ–‡ä»¶æ”¾åœ¨äº†æ ¹ç›®å½•ä¸‹ï¼Œä»£ç å¦‚ä¸‹ï¼š</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
Model_Local_Path = "D:\llm\THUDM\chatglm2-6b-int4"
            </pre>
            <h4>æœ¬åœ°æ¨¡å‹å’ŒCPUéƒ¨ç½²</h4>
            <p>ä¸ºäº†æ”¯æŒæœ¬åœ°æ¨¡å‹å’ŒCPUéƒ¨ç½²ï¼Œéœ€è¦ä¿®æ”¹ä¸€ä¸‹ä»£ç ã€‚æ¨¡å‹æ”¯æŒä¸‰ç§è®¿é—®æ¨¡å¼ï¼šå‘½ä»¤è¡Œã€APIã€WebUIã€‚å…ˆç”¨WebUIæµ‹è¯•ä¸€ä¸‹ï¼Œå±•ç¤ºæ•ˆæœæ¯”è¾ƒå¥½ã€‚ä¿®æ”¹<line-code>web_demo2.py</line-code>æ–‡ä»¶ä»£ç å¦‚ä¸‹ï¼š</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
#   å¼•å…¥æœ¬åœ°æ¨¡å‹ç»å¯¹è·¯å¾„
from modelPath import Model_Local_Path
            </pre>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
#   åŸå§‹ä»£ç 
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).cuda()

#   ç”¨ä¸‹é¢çš„ä¸¤è¡Œä»£ç æ›¿æ¢ä¸Šé¢çš„
#       .float() æ”¯æŒCPU
tokenizer = AutoTokenizer.from_pretrained(Model_Local_Path, trust_remote_code=True)
model = AutoModel.from_pretrained(Model_Local_Path, trust_remote_code=True).float()
            </pre>
            <h3>è™šæ‹Ÿç¯å¢ƒ</h3>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
#   åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
#   æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
.\venv\scripts\activate
            </pre>
            <p>
                <img src="./image/3.png" alt="Python è™šæ‹Ÿç¯å¢ƒ" />
            </p>
            <h3>å®‰è£…ä¾èµ–</h3>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
#   æˆ‘è¿™é‡Œè®¾ç½®å…¨å±€çš„æ¸…åé•œåƒæº
pip install -r requirements.txt
#   å¦‚æœä½ ä¸æƒ³å…¨å±€è®¾ç½®ï¼Œä¹Ÿå¯ä»¥ä»…æœ¬æ¬¡å®‰è£…æ—¶ä½¿ç”¨é•œåƒæºï¼ˆå¦‚ä¸‹é¢çš„æˆªå›¾ï¼‰
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
            </pre>
            <p>
                <img src="./image/4.png" alt="ChatGLM2-6B å®‰è£…ä¾èµ–" />
            </p>
            <h3>è·‘èµ·æ¥</h3>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
#   å‘½ä»¤è¡Œ Demo
python cli_demo.py
#   åŸºäº Gradio çš„ç½‘é¡µç‰ˆ demo       ã€æ³¨æ„ã€‘é—®ç­”æ—¶å‡ºç°bugï¼Œå‰ç«¯æ“ä½œdomçš„é—®é¢˜
python web_demo.py
#   åŸºäº Streamlit çš„ç½‘é¡µç‰ˆ demo
streamlit run web_demo2.py
            </pre>
            <p>è¿™é‡Œè¿è¡Œçš„æ˜¯åŸºäº Streamlit çš„ç½‘é¡µç‰ˆã€‚å‘½ä»¤è¡Œä¼šå‡ºç°ä¸€äº›é”™è¯¯ï¼Œä½†æ˜¯å¹¶ä¸å½±å“è¿è¡Œã€‚æ•ˆæœå›¾å¦‚ä¸‹ï¼š</p>
            <p>
                <img src="./image/5.png" alt="ChatGLM2-6B å‘½ä»¤è¡Œ streamlit run web_demo2.py è¿è¡Œç»“æœã€‚" />
            </p>
            <p>
                <img src="./image/6.png" alt="ChatGLM2-6B WebUIç•Œé¢æ•ˆæœã€‚" />
            </p>
            <h3>èµ¶ç´§èŠèŠå§</h3>
            <p>
                <img src="./image/7.png" alt="ChatGLM2-6B WebUIé—®ç­”ï¼šä½ å¥½ï¼› javascript å®ç° æ–æ³¢é‚£å¥‘æ•°åˆ—" />
            </p>
            <p>è¿™å®¶ä¼™ç”Ÿæˆçš„ä»£ç ï¼Œå¾ˆæœ‰éšæœºæ€§ã€‚è¿™æ¬¡å›ç­”çš„å’Œä¸Šæ¬¡å·®å¼‚å¾ˆå¤§å•Šï¼æˆ‘éƒ½ä¸å¥½æ„æ€æˆªå›¾äº†ã€‚ä¸è¿‡ï¼Œå’±ä¹Ÿå¾—ä½“è°…å•Šï¼Œæ¯•ç«Ÿè¿™æ˜¯æ”¹ç‰ˆå•Šï¼Œä½ è¯´æ˜¯å§ã€‚ä¸‹é¢çš„æˆªå›¾æ˜¯å‰å‡ å¤©ç¬¬ä¸€æ¬¡è·‘é€šæ—¶çš„æˆªå›¾ï¼š</p>
            <p>
                <img src="./image/8.png" alt="ChatGLM2-6B WebUIé—®ç­”ï¼Œjavascript å®ç° æ–æ³¢é‚£å¥‘æ•°åˆ—" />
            </p>
            <h2>å¼€å§‹ç ç –</h2>
            <p>ä¸Šé¢åŸºäºå‘½ä»¤è¡Œã€Web UIç®€å•çš„ä½“éªŒäº†ä¸€ä¸‹ã€‚å’±è‚¯å®šä¸èƒ½æ­¢æ­¥äºæ­¤å•Šï¼Œæ˜¯å§ï¼èµ¶ç´§æ“ä½œèµ·æ¥å§ï¼</p>
            <h3>ç¤ºä¾‹1ï¼šChatGLM2-6B/api.py + langchain</h3>
            <h4>æœåŠ¡å™¨ç«¯</h4>
            <p>é¡¹ç›®ä¸­å·²ç»æä¾›äº†<line-code>ChatGLM2-6B/api.py</line-code>ï¼Œç”¨å®ƒæ¥ç»ƒç»ƒæ‰‹ã€‚çœ‹çœ‹éƒ½æ”¹äº†å“ªäº›ä»£ç ï¼š</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
if __name__ == '__main__':
    Model_Local_Path = "D:\llm\THUDM\chatglm2-6b-int4"
    tokenizer = AutoTokenizer.from_pretrained(Model_Local_Path, trust_remote_code=True)
    model = AutoModel.from_pretrained(Model_Local_Path, trust_remote_code=True).float()
    # å¤šæ˜¾å¡æ”¯æŒï¼Œä½¿ç”¨ä¸‹é¢ä¸‰è¡Œä»£æ›¿ä¸Šé¢ä¸¤è¡Œï¼Œå°†num_gpusæ”¹ä¸ºä½ å®é™…çš„æ˜¾å¡æ•°é‡
    # model_path = "THUDM/chatglm2-6b"
    # tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    # model = load_model_on_gpus(model_path, num_gpus=2)
    model.eval()
    uvicorn.run(app, host='127.0.0.1', port=8899, workers=1)
            </pre>
            <p>å¥½äº†ï¼Œä»£ç å¼„å¥½äº†ï¼Œèµ¶ç´§è®©APIè·‘èµ·æ¥å§ï¼è¯·æ‚¨è¾“å…¥å¯åŠ¨å‘½ä»¤ï¼š</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
#   API Demo
python api.py
            </pre>
            <p>
                <img src="./image/9.png" alt="ChatGLM2-6B python api.py" />
            </p>
            <h4>å®¢æˆ·ç«¯</h4>
            <p>APIè·‘èµ·æ¥ä¹‹åï¼Œå…ˆç”¨å¤§åé¼é¼çš„<line-code>langchain</line-code>å°è¯•ä¸€ä¸‹ã€‚</p>
            <details>
                <summary>ç¬¬ä¸€ä¸ªä¾‹å­</summary>
                <p>å…·ä½“ä»£ç å¦‚ä¸‹ï¼š</p>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
import traceback

from langchain.prompts import PromptTemplate
from langchain.llms.chatglm import ChatGLM
from langchain.chains import LLMChain

template = """{question}"""
prompt = PromptTemplate(template=template, input_variables=["question"])
llm = ChatGLM(
    endpoint_url="http://127.0.0.1:8899",
    max_token=80000,
    history=[],
    top_p=0.9,
    model_kwargs={"sample_model_args": False},
)
llm_chain = LLMChain(prompt=prompt, llm=llm)

try:
    question1 = "ä½ çš„åå­—æ˜¯ä»€ä¹ˆå•Š"
    print("é—®ï¼š" + question1)
    answer1 = llm_chain.run(question1)
    print("ç­”ï¼š" + str(answer1))

    question2 = "ä½ çŸ¥é“ xiaodu114 å—"
    print("é—®ï¼š" + question2)
    answer2 = llm_chain.run(question2)
    print("ç­”ï¼š" + str(answer2))
except Exception as ex:
    print(ex.args)
    print("="*24+">")
    print(traceback.format_exc())
                </pre>
            </details>
            <details>
                <summary>ç¬¬äºŒä¸ªä¾‹å­</summary>
                <p>å…·ä½“ä»£ç å¦‚ä¸‹ï¼š</p>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
import traceback

from langchain.llms.chatglm import ChatGLM
from langchain.chains.question_answering import load_qa_chain

llm = ChatGLM(
    endpoint_url="http://127.0.0.1:8899",
    max_token=80000,
    history=[],
    top_p=0.9,
    model_kwargs={"sample_model_args": False},
)
chain = load_qa_chain(llm, chain_type="stuff")

question = "ä½ çŸ¥é“ xiaodu114 å—ï¼Ÿ"

try:
    print("é—®ï¼š" + question)
    answer = chain.run(input_documents=[], question=question)
    print("ç­”ï¼š" + str(answer))
except Exception as ex:
    print(ex.args)
    print("="*24+">")
    print(traceback.format_exc())
                </pre>
            </details>
            <p>ä¸Šé¢çš„ä¸¤ä¸ªä¾‹å­åˆ†åˆ«ä½¿ç”¨ï¼šLLMChain å’Œ load_qa_chainï¼Œåœ¨ç»“æœä¸Šæœ‰æ‰€ä¸åŒï¼Œå›ç­”ç»“æœå’Œæ—¥å¿—ä¿¡æ¯ä¹Ÿæœ‰äº›ä»¤äººåƒæƒŠï¼Œä¸‹é¢æ˜¯ç»“æœï¼š</p>
            <p>
                <img src="./image/10.png" alt="ChatGLM2-6B api.py langchain + load_qa_chain" />
            </p>
            <h3>ç¤ºä¾‹2ï¼šChatGLM2-6B/openai_api.py + javascript</h3>
            <h4>æœåŠ¡å™¨ç«¯</h4>
            <p>è¿™ä¸ªç¤ºä¾‹ä½¿ç”¨<line-code>ChatGLM2-6B/openai_api.py</line-code>ã€‚ä»ç„¶ä¿®æ”¹ä¸€ä¸‹æ¨¡å‹è·¯å¾„å’ŒCPUæ”¯æŒï¼Œå…·ä½“çš„å’Œç¤ºä¾‹1ç›¸åŒï¼Œè¿™é‡Œå°±ä¸å±•ç¤ºäº†ã€‚ä¸‹é¢è¯·æ‚¨è¾“å…¥å¯åŠ¨å‘½ä»¤ï¼š</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
#   API Demoï¼ˆæ¨¡æ‹Ÿ openai apiï¼‰
python openai_api.py
            </pre>
            <p>
                <img src="./image/11.png" alt="ChatGLM2-6B python openai_api.py" />
            </p>
            <h4>å®¢æˆ·ç«¯</h4>
            <details>
                <summary>javascript è°ƒç”¨ä»£ç </summary>
                <p>å…·ä½“ä»£ç å¦‚ä¸‹ï¼š</p>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="javascript">
const btnOpenAiAPIEle = document.getElementById("btnOpenAiAPI");
btnOpenAiAPIEle.addEventListener("click", () => {
    let myHeaders = new Headers();
    myHeaders.append("Content-Type", "application/json");
    myHeaders.append("Accept", "text/event-stream");

    const url = "http://127.0.0.1:8899/v1/chat/completions";
    fetch(url, {
        method: "POST",
        headers: myHeaders,
        body: JSON.stringify({
            model: "xxx",
            stream: true,
            messages: [{ role: "user", content: "ä½ å¥½" }]
        })
    }).then(
        (response) => {
            if (!(response && response.ok)) {
                console.error("å¼‚å¸¸ï¼š" + response.status);
                return;
            }
            let reader = response.body.getReader();
            reader.read().then(function processResult(result) {
                if (result.done) {
                    console.log("Event stream ended.");
                    return;
                }
                const decoder = new TextDecoder();
                const decodedString = decoder.decode(result.value);
                console.log(decodedString);
                // ç»§ç»­è¯»å–ä¸‹ä¸€ä¸ªäº‹ä»¶æµæ•°æ®
                reader.read().then(processResult);
            });
        },
        (error) => {
            console.error(`å¼‚å¸¸ï¼š${JSON.stringify(error)}`);
        }
    );
});
                </pre>
            </details>
            <p>æœç„¶æ˜¯ä¸é¡ºåˆ©å•Šï¼è¿™å¼‚å¸¸å¯çœŸé•¿â€¦â€¦</p>
            <p>
                <img src="./image/12.png" alt="ChatGLM2-6B python openai_api.py TypeError: `dumps_kwargs` keyword arguments are no longer supported." />
            </p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
#   ä¿®æ”¹ä¹‹å‰çš„ä»£ç ã€‚å¯ä»¥ç”¨ä¸‹é¢çš„ä»£ç æ›¿æ¢ã€‚è¿™é‡Œå…±ä¿®æ”¹äº†ä¸‰å¤„
"{}".format(chunk.json(exclude_unset=True, ensure_ascii=False))
#   ä¿®æ”¹ä¹‹å
"{}".format(chunk.model_dump_json(exclude_unset=True))
            </pre>
            <p>ä¿®æ”¹å®Œä»£ç ä¹‹åï¼Œé‡æ–°æ‰§è¡Œ<line-code>python openai_api.py</line-code>ï¼Œè¿™å›å°±æ¯”è¾ƒå—¨çš®äº†ï¼š</p>
            <p>
                <img src="./image/13.png" alt="ChatGLM2-6B python openai_api.py å‰ç«¯JSè°ƒç”¨" />
            </p>
            <h3>ç¤ºä¾‹3ï¼šæœåŠ¡å™¨ Hello World</h3>
            <p>é‡‡ç”¨<line-code>fastapi</line-code>+<line-code>uvicorn</line-code>çš„æ–¹å¼â€¦â€¦</p>
            <h4>æœåŠ¡å™¨ç«¯</h4>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
import datetime
import uvicorn
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/GetDateTime")
def get_server_datetime():
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

if __name__ == '__main__':
    uvicorn.run(app, host='127.0.0.1', port=8899, workers=1)
            </pre>
            <p>å¯åŠ¨ä¸€ä¸ªæœåŠ¡å™¨ä¸è¦å¤ªæ–¹ä¾¿å“¦ï¼è¿˜è´´å¿ƒçš„å¸¦ä¸Šäº†APIæ–‡æ¡£ï¼Œç®€ç›´ä¸è¦å¤ªçˆ½å•Šï¼è¯·çœ‹ï¼š</p>
            <p>
                <img src="./image/14.png" alt="fastapi ä¹‹ è·å–æœåŠ¡å™¨æ—¶é—´" />
            </p>
            <h4>å®¢æˆ·ç«¯</h4>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="javascript">
const btnGetDateTimeEle = document.getElementById("btnGetDateTime");
btnGetDateTimeEle.addEventListener("click", () => {
    fetch("http://127.0.0.1:8899/GetDateTime")
        .then(
            (response) => {
                if (response.ok) {
                    return response.json();
                } else {
                    console.error(`å¼‚å¸¸ï¼å“åº”çŠ¶æ€ç ï¼š${response.status} ï¼›å“åº”çŠ¶æ€ä¿¡æ¯ï¼š${response.statusText}`);
                }
            },
            (error) => {
                console.error(`å¼‚å¸¸ï¼ ${JSON.stringify(error)}`);
            }
        )
        .then((data) => {
            alert("æœåŠ¡å™¨ç«¯æ—¶é—´ï¼š" + data);
        });
});
            </pre>
            <h3>ç¤ºä¾‹4ï¼šå·å¸ˆ openai_api.py</h3>
            <p>è¿™é‡Œæƒ³ç€å­¦ä¹ ä¸€ä¸‹<line-code>openai_api.py</line-code>ï¼Œè‡ªå·±ä¹Ÿè¯•ç€å†™ä¸€ä¸ªç®€å•çš„æµå¼å“åº”çš„æ¥å£å¹¶ä¸”æ”¯æŒå¯¹è¯å†å²ï¼Œå¹¶ä¸”â€¦â€¦ï¼Œå¥½å§ï¼Œæ²¡æœ‰äº†</p>
            <h4>æœåŠ¡å™¨ç«¯</h4>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
import datetime
import json
import traceback
import uvicorn
from transformers import AutoTokenizer, AutoModel
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from sse_starlette.sse import EventSourceResponse

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/GetDateTime")
def get_server_datetime():
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

async def process(prompt, max_length, top_p, temperature, history):
    """
    å‘æ–‡å­—æ¶ˆæ¯
    """
    # ä¸èƒ½æ˜¯ç©ºæ¶ˆæ¯
    if not prompt:
        yield "prompt ä¸èƒ½ä¸ºç©º"
        return

    try:
        current_length = 0
        #   ç»æµ‹è¯•å‘ç°ï¼Œå†å²å¯¹è¯ä¹Ÿå°±æ˜¯ history å‚æ•°éœ€è¦çš„æ ¼å¼å¦‚ä¸‹ï¼š
        #       [('ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œ', 'ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚'), ('ä»–çš„é¢ç§¯æ˜¯å¤šå°‘', 'åŒ—äº¬å¸‚çš„æ€»é¢ç§¯å¤§çº¦ä¸º16,800å¹³æ–¹å…¬é‡Œã€‚')]
        for response, history in model.stream_chat(tokenizer,
                                                   prompt, 
                                                   history=history,
                                                   max_length=max_length if max_length else 999999,
                                                   top_p=top_p if top_p else 0.7,
                                                   temperature=temperature if temperature else 0.95):
            if len(response) == current_length:
                continue
            new_text = response[current_length:]
            current_length = len(response)
            yield json.dumps({"text":str(new_text)},ensure_ascii=False)
    except Exception as ex:
        yield ex.args +"\n" + traceback.format_exc()
        return

@app.post("/ChatStream")
async def create_item(request: Request):
    global model, tokenizer
    json_post_raw = await request.json()
    json_post = json.dumps(json_post_raw)
    json_post_list = json.loads(json_post)

    prompt = json_post_list.get('prompt')
    max_length = json_post_list.get('max_length')
    top_p = json_post_list.get('top_p')
    temperature = json_post_list.get('temperature')
    history = json_post_list.get('history')
    #   å…ˆè¿™æ ·å®šä¸€ä¸‹å®¢æˆ·ç«¯ä¼ å…¥ history çš„æ ¼å¼ï¼Œå¦‚ä¸‹ï¼š
    #       [["ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œ", "ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚"],["ä»–çš„é¢ç§¯æ˜¯å¤šå°‘", "åŒ—äº¬å¸‚çš„æ€»é¢ç§¯å¤§çº¦ä¸º16,800å¹³æ–¹å…¬é‡Œã€‚"]]
    #   è¿™é‡Œåœ¨è½¬æˆ model.stream_chat éœ€è¦çš„æ ¼å¼
    history = [tuple(h) for h in history] 

    answer_text = process(prompt, max_length, top_p, temperature,history)
    return EventSourceResponse(answer_text, media_type="text/event-stream")


if __name__ == '__main__':
    model_path = "D:\llm\THUDM\chatglm2-6b-int4"
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModel.from_pretrained(model_path, trust_remote_code=True).float()

    model = model.eval()
    uvicorn.run(app, host='127.0.0.1', port=8899, workers=1)
                </pre>
            </details>
            <h4>å®¢æˆ·ç«¯</h4>
            <p>è¿™é‡Œè¿˜æ˜¯ä½¿ç”¨<line-code>JavaScript</line-code>è°ƒç”¨çš„ï¼Œæ¯•ç«Ÿä»¥åâ€¦â€¦ä½ è¯´æ˜¯å§</p>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="javascript">
const btnTest1Ele = document.getElementById("btnTest1");
btnTest1Ele.addEventListener("click", () => {
    let myHeaders = new Headers();
    myHeaders.append("Content-Type", "application/json");
    myHeaders.append("Accept", "text/event-stream");

    //  é™†ç»­è¯¢é—®ä¸‹é¢è¿™å‡ ä¸ªé—®é¢˜ï¼š
    //  ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œ
    //  ä»–çš„é¢ç§¯æ˜¯å¤šå°‘
    //  ä»–æœ‰æœºåœºå—ï¼Ÿå‡ åº§ï¼Ÿ
    //  ä¸‹è¾–å“ªäº›è¡Œæ”¿åŒºåŸŸ
    const url = "http://127.0.0.1:8899/ChatStream";
    fetch(url, {
        method: "POST",
        headers: myHeaders,
        // body: JSON.stringify({
        //     prompt: "ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œ",
        //     history: []
        // })
        // body: JSON.stringify({
        //     prompt: "ä»–çš„é¢ç§¯æ˜¯å¤šå°‘",
        //     history: [["ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œ", "ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚"]]
        // })
        body: JSON.stringify({
            prompt: "ä»–æœ‰æœºåœºå—ï¼Ÿå‡ åº§ï¼Ÿ",
            history: [
                ["ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œ", "ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚"],
                ["ä»–çš„é¢ç§¯æ˜¯å¤šå°‘", "åŒ—äº¬å¸‚çš„æ€»é¢ç§¯å¤§çº¦ä¸º16,800å¹³æ–¹å…¬é‡Œã€‚"]
            ]
        })
    }).then(
        (response) => {
            if (!(response && response.ok)) {
                console.error("å¼‚å¸¸ï¼š" + response.status);
                return;
            }
            let answer = "";
            let reader = response.body.getReader();
            const decoder = new TextDecoder();
            reader.read().then(function processResult(result) {
                if (result.done) {
                    console.log("Event stream ended.");
                    console.log(answer);
                    return;
                }
                try {
                    let decodedString = decoder.decode(result.value);
                    console.log(decodedString);
                    if (decodedString.startsWith("data:")) {
                        let data = JSON.parse(decodedString.slice("data:".length).trim());
                        answer += data.text;
                    }
                } catch (error) {}

                // ç»§ç»­è¯»å–ä¸‹ä¸€ä¸ªäº‹ä»¶æµæ•°æ®
                reader.read().then(processResult);
            });
        },
        (error) => {
            console.error(`å¼‚å¸¸ï¼š${JSON.stringify(error)}`);
        }
    );
});
                </pre>
            </details>
            <p>ä¸è¡Œï¼Œæˆ‘å¾—æˆªä¸ªå›¾ï¼Œè®©ä½ ä»¬çœ‹çœ‹æ•ˆæœï¼š</p>
            <p>
                <img src="./image/15.png" alt="fastapi ä¹‹ è·å–æœåŠ¡å™¨æ—¶é—´" />
            </p>
            <h2>å¾®è°ƒ</h2>
            <p>è¿™æ‰æ˜¯ç¡¬èœã€‚åœ¨é€šç”¨å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œæ·»åŠ äº›è‡ªå·±ç‰¹è‰²çš„è¯­æ–™ï¼Œè®©å…¶æˆä¸ºä½ çš„è´´å¿ƒå°åŠ©æ‰‹ã€‚</p>
            <p>ç†æƒ³å¾ˆä¸°æ»¡ï¼Œæ˜¾ç¤ºå¾ˆè‹—æ¡ã€‚æš‚æ—¶æ²¡æœ‰è°ƒé€šï¼Œè¿˜å¾—ç»§ç»­åƒå•Šï¼å¾—èµ¶ç´§èƒ–èµ·æ¥ã€‚</p>
            <p>è®°å½•ä¸€ä¸‹æˆ‘çš„å¾®è°ƒè¿‡ç¨‹ï¼š</p>
            <dl>
                <dt>å®‰è£…ä¾èµ–</dt>
                <dd>pip install rouge_chinese nltk jieba datasets</dd>
                <dt>å‡†å¤‡æ•°æ®é›†</dt>
                <dd>å¯ä»¥æŒ‰ç…§å®˜ç½‘ä¸Šçš„å…³äºå¹¿å‘Šçš„ï¼Œä¹Ÿå¯ä»¥è‡ªå·±å¼„ç‚¹ã€‚æ„Ÿè§‰å¹¿å‘Šçš„æœ‰ç‚¹å¤§ï¼Œæ€•æŠŠæˆ‘è¿™è€æœ‹å‹ç´¯ç€ï¼Œè¿™é‡Œå‚è€ƒçš„æ˜¯ï¼š<a href="https://zhuanlan.zhihu.com/p/643531454" target="_blank">LangChain + ChatGLM2-6B æ­å»ºä¸ªäººä¸“å±çŸ¥è¯†åº“ - çŸ¥ä¹</a></dd>
                <dt>ä¿®æ”¹ main.py æ–‡ä»¶</dt>
                <dd>ä»…ä¾é CPUï¼Œéœ€è¦ä¿®æ”¹ä¸€ä¸‹ã€‚CPUä¸æ”¯æŒ<line-code>half</line-code></dd>
                <dt>ç¦ç”¨ W&B</dt>
                <dd>æ§åˆ¶å°ï¼š export WANDB_DISABLED=true</dd>
                <dd>ã€è¯´æ˜ã€‘å¦‚æœä¸ç¦ç”¨å¯èƒ½ä¼šä¸­æ–­å¾®è°ƒè®­ç»ƒï¼Œä»¥é˜²ä¸‡ä¸€ï¼Œè¿˜æ˜¯ç¦äº†å§</dd>
                <dt>.shè½¬.bat</dt>
                <dd>è¿è¡Œ .shæ–‡ä»¶ä¼šæœ‰é—®é¢˜ï¼Œå“ªæ€•å·²ç»å®‰è£… git ã€‚</dd>
                <dd>è½¬.batæ–‡ä»¶ï¼Œè¿™é‡Œå‚è€ƒçš„æ˜¯ï¼š<a href="https://blog.csdn.net/oddrock/article/details/132230680" target="_blank">ChatGLM2-6Båœ¨Windowsä¸‹çš„å¾®è°ƒ_è±ªæ°ç¬‘å¼€æ€€çš„åšå®¢-CSDNåšå®¢</a></dd>
                <dd>
                    <dl>
                        <dt>å®˜æ–¹å‚æ•°è§£è¯´</dt>
                        <dd>train.sh ä¸­çš„ PRE_SEQ_LEN å’Œ LR åˆ†åˆ«æ˜¯ soft prompt é•¿åº¦å’Œè®­ç»ƒçš„å­¦ä¹ ç‡ï¼Œå¯ä»¥è¿›è¡Œè°ƒèŠ‚ä»¥å–å¾—æœ€ä½³çš„æ•ˆæœã€‚</dd>
                        <dd>P-Tuning-v2 æ–¹æ³•ä¼šå†»ç»“å…¨éƒ¨çš„æ¨¡å‹å‚æ•°ï¼Œå¯é€šè¿‡è°ƒæ•´ quantization_bit æ¥è¢«åŸå§‹æ¨¡å‹çš„é‡åŒ–ç­‰çº§ï¼Œä¸åŠ æ­¤é€‰é¡¹åˆ™ä¸º FP16 ç²¾åº¦åŠ è½½ã€‚</dd>
                        <dd>åœ¨é»˜è®¤é…ç½® quantization_bit=4ã€per_device_train_batch_size=1ã€gradient_accumulation_steps=16 ä¸‹ï¼Œ INT4 çš„æ¨¡å‹å‚æ•°è¢«å†»ç»“ï¼Œä¸€æ¬¡è®­ç»ƒè¿­ä»£ä¼šä»¥ 1 çš„æ‰¹å¤„ç†å¤§å°è¿›è¡Œ 16 æ¬¡ç´¯åŠ çš„å‰åå‘ä¼ æ’­ï¼Œç­‰æ•ˆä¸º 16 çš„æ€»æ‰¹å¤„ç†å¤§å°ï¼Œæ­¤æ—¶æœ€ä½åªéœ€ 6.7G æ˜¾å­˜ã€‚ è‹¥æƒ³åœ¨åŒç­‰æ‰¹å¤„ç†å¤§å°ä¸‹æå‡è®­ç»ƒæ•ˆç‡ï¼Œå¯åœ¨äºŒè€…ä¹˜ç§¯ä¸å˜çš„æƒ…å†µä¸‹ï¼ŒåŠ å¤§ per_device_train_batch_size çš„å€¼ï¼Œä½†ä¹Ÿä¼šå¸¦æ¥æ›´å¤šçš„æ˜¾å­˜æ¶ˆè€—ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µé…Œæƒ…è°ƒæ•´ã€‚</dd>
                    </dl>
                </dd>
                <dt>å¾®è°ƒæ—¶é—´å¤ªé•¿äº†</dt>
                <dd>å‚æ•° max_steps å¿…é¡»å¤§äº save_steps ã€‚ä»–ä»¬å“¥ä¿©çš„å€¼è¶Šå¤§æ—¶é—´è¶Šé•¿â€¦â€¦</dd>
            </dl>
            <p>é©å‘½å°šæœªæˆåŠŸï¼Œè¿˜å¾—ç»§ç»­æå•Šï¼æˆ‘çš„å¾®è°ƒâ€¦â€¦</p>
        </div>
    </body>
</html>
