<!DOCTYPE html>
<html lang="zh-cmn-Hans">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta http-equiv="X-UA-Compatible" content="ie=edge" />
        <title>Xorbits Inference - xiaodu114.github.io</title>
        <meta name="keywords" content="Xorbits Inference,,,,,,,,,,," />
        <meta name="description" content="官方介绍：Xorbits Inference（Xinference）是一个性能强大且功能全面的分布式推理框架。可用于大语言模型（LLM），语音识别模型，多模态模型等各种模型的推理。通过 Xorbits Inference，你可以轻松地一键部署你自己的模型或内置的前沿开源模型。无论你是研究者，开发者，或是数据科学家，都可以通过 Xorbits Inference 与最前沿的 AI 模型，发掘更多可能。" />

        <script src="/p/_/js/main.js"></script>
        <script src="../_common/main.js"></script>
    </head>

    <body>
        <!-- github访问地址：/p/llm/xorbitsInference/index.html -->
        <div class="blog-page">
            <h1>Xorbits Inference</h1>
            <p>官网：<a href="https://github.com/xorbitsai/inference" target="_blank">GitHub - xorbitsai/inference</a></p>
            <p>官方介绍：Xorbits Inference（Xinference）是一个性能强大且功能全面的分布式推理框架。可用于大语言模型（LLM），语音识别模型，多模态模型等各种模型的推理。通过 Xorbits Inference，你可以轻松地一键部署你自己的模型或内置的前沿开源模型。无论你是研究者，开发者，或是数据科学家，都可以通过 Xorbits Inference 与最前沿的 AI 模型，发掘更多可能。</p>
            <p>官方文档：<a href="https://inference.readthedocs.io/zh-cn/latest/index.html" target="_blank">欢迎来到 Xinference！ — Xinference</a></p>
            <mark-block explain="说明">
                <p>运行环境：Ubuntu 22.04 + Intel(R)Xeon(R) CPU E5-2699 v4 2.20GHZ + RAM（512GB）</p>
            </mark-block>
            <h2>前期准备</h2>
            <h3>下载项目</h3>
            <p><strong>2024-04-09</strong> 这里下载的是<line-code>v0.10.0</line-code>版本，本地目录如下：</p>
            <p>
                <img src="./image/1.png" alt="xorbitsai/inference 项目目录结构" />
            </p>
            <h3>虚拟环境和依赖</h3>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
#   创建虚拟环境并激活
python3 -m venv venv
source ./venv/bin/activate

#   安装依赖
pip install "xinference[all]"
            </pre>
            <h2>运行 Xinference</h2>
            <p>上面准备好后就可以启动了，启动命令如下：</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
HF_ENDPOINT=https://hf-mirror.com \
XINFERENCE_MODEL_SRC=modelscope \
XINFERENCE_HOME=/home/xxx/llm/0-model \
xinference-local --host 192.168.xxx.xxx --port 9997
            </pre>
            <mark-block>
                <p>环境变量或者更多参数请参考官方文档</p>
                <ul>
                    <li>
                        <a href="https://inference.readthedocs.io/zh-cn/latest/getting_started/environments.html" target="_blank">环境变量 — Xinference</a>
                    </li>
                    <li>
                        <a href="https://inference.readthedocs.io/zh-cn/latest/models/sources/sources.html" target="_blank">模型来源 — Xinference</a>
                    </li>
                </ul>
            </mark-block>
            <p>
                <img src="./image/2.png" alt="xorbitsai/inference 项目启动效果图" />
            </p>
            <p>接着我们再看一下Web 页面的效果，如下：</p>
            <p>
                <img src="./image/3.png" alt="xorbitsai/inference 项目，Web页面效果图" />
            </p>
            <p>让我们再看一下他的接口文档页面，看一下都提供了哪些接口：</p>
            <p>
                <img src="./image/4.png" alt="xorbitsai/inference 项目，接口文档页面" />
            </p>
            <h2>Embedding 模型</h2>
            <p>已经下载了几个<line-code>Embedding</line-code>模型，如下图：</p>
            <p>
                <img src="./image/5.png" alt="xorbitsai/inference 项目，已下载Embedding 模型截图" />
            </p>
            <h3>xinference.client</h3>
            <mark-block>
                <p>下面的代码来自：<a href="https://inference.readthedocs.io/zh-cn/latest/user_guide/client_api.html" target="_blank">客户端 API — Xinference</a></p>
            </mark-block>
            <p>这里采用<line-code>xinference.client</line-code>方式测试。在根目录下创建文件<line-code>1.py</line-code>，代码如下：</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
from xinference.client import Client

client = Client("http://192.168.xxx.xxx:9997")
model_uid = client.launch_model(model_name="jina-embeddings-v2-base-zh", model_type="embedding")
model = client.get_model(model_uid)

input_text = "What is the capital of China?"
output = model.create_embedding(input_text)
print(output)
            </pre>
            <p></p>
            <p>运行结果如下图：</p>
            <p>
                <img src="./image/6.png" alt="xinference.client 方式调用Embedding 模型" />
            </p>
            <h3>Postman</h3>
            <p>再使用<line-code>Postman</line-code>直接调用API测试一下，如图：</p>
            <p>
                <img src="./image/7.png" alt="Postman 方式调用Embedding 模型" />
            </p>
        </div>
    </body>
</html>
