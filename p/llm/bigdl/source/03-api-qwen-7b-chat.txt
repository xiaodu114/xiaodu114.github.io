import sys
sys.path.append(".")

from typing import List,Dict,Tuple
import json
from transformers import AutoTokenizer
from bigdl.llm.transformers import AutoModelForCausalLM
import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from sse_starlette.sse import EventSourceResponse

from config.model import model_path_dict
from models.openai import ChatMessage, ChatCompletionRequest

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

async def chat_stream(query: str, history: List[Tuple[str, str]], gen_kwargs: Dict):
    current_length = 0
    response_generator = model.chat_stream(tokenizer, query, history=history, **gen_kwargs)
    for new_response in response_generator:
        if len(new_response) == current_length:
            continue
        new_text = new_response[current_length:]
        current_length = len(new_response)
        yield json.dumps({"text":str(new_text)},ensure_ascii=False)
    yield "[DONE]"

def chat(messages:List[ChatMessage], system: str = "You are a helpful assistant."):
    im_start, im_end = "<|im_start|>", "<|im_end|>"

    prompt = f"{im_start}system\n{system}{im_end}\n"
    for message in messages:
        match message.role:
            case "user":
                prompt += f"{im_start}user\n{message.content}{im_end}\n" 
            case "assistant":
                prompt += f"{im_start}assistant\n{message.content}{im_end}\n"
    prompt += f"{im_start}assistant\n"
    input_ids = tokenizer.encode(prompt, return_tensors="pt")  
    outputs = model.generate(input_ids,
                             do_sample=True,
                             max_new_tokens=1024,
                             top_p=0.8,
                             temperature=0.8)
    response = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)
    return response


@app.post("/chat")
async def create_chat_completion(request: ChatCompletionRequest):
    if (len(request.messages) == 0) :
        raise HTTPException(status_code=400, detail="messages长度不能为0")
    if (request.messages[-1].role != "user"):
        raise HTTPException(status_code=400, detail="messages最后一项的role必须为user")

    prompt = request.messages[-1].content
    history_messages = request.messages[:-1]
    history = []
    if len(history_messages) % 2 == 0:
        for i in range(0, len(history_messages), 2):
            if history_messages[i].role == "user" and history_messages[i+1].role == "assistant":
                history.append((history_messages[i].content, history_messages[i+1].content))

    if (request.stream):
        generate = chat_stream(prompt, history, {})
        return EventSourceResponse(generate, media_type="text/event-stream")
    else:
        return chat(request.messages)
    

if __name__ == '__main__':
    model_path = model_path_dict["Qwen"]["7B-Chat"]
    global model, tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_path, load_in_4bit=True, trust_remote_code=True)
    uvicorn.run(app, host='IP地址', port=6666, workers=1)