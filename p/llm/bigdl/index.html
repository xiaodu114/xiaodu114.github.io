<!DOCTYPE html>
<html lang="zh-cmn-Hans">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta http-equiv="X-UA-Compatible" content="ie=edge" />
        <title>BigDL-LLM - xiaodu114.github.io</title>
        <meta name="keywords" content="LLM,å¤§è¯­è¨€æ¨¡å‹,BigDL,BigDL-LLM,Baichuan2-7B-Chat,chatglm2-6b,chatglm2-3b,Qwen-7B-Chat,Yi-6B-Chat" />
        <meta name="description" content="BigDL-LLM æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–åŠ é€Ÿåº“ã€‚è¿™é‡Œè®°å½•ä¸€ä¸‹å¯¹ä»–çš„ä½“éªŒâ€¦â€¦" />

        <script src="/p/_/js/main.js"></script>
        <script src="../_common/main.js"></script>
    </head>

    <body>
        <!-- githubè®¿é—®åœ°å€ï¼š/p/llm/bigdl/index.html -->
        <div class="blog-page">
            <h1>BigDL-LLM</h1>
            <p>GitHubå®˜ç½‘ï¼š<a href="https://github.com/intel-analytics/BigDL" target="_blank">GitHub - intel-analytics/BigDL: Accelerate LLM with low-bit (FP4 / INT4 / FP8 / INT8) optimizations using bigdl-llm</a></p>
            <p>å®˜ç½‘æ•™ç¨‹ï¼š<a href="https://github.com/intel-analytics/bigdl-llm-tutorial" target="_blank">GitHub - intel-analytics/bigdl-llm-tutorial: Accelerate LLM with low-bit (FP4 / INT4 / FP8 / INT8) optimizations using bigdl-llm</a></p>
            <p>æ ¼å±€å°äº†ï¼ LLM åŠŸèƒ½åªæ˜¯ BigDL çš„ä¸€éƒ¨åˆ†ï¼ŒåŸä»¥ä¸ºåªæ˜¯å…³äºå¤§è¯­è¨€æ¨¡å‹çš„ã€‚è‹±ç‰¹å°”å‡ºå“</p>
            <p>è¯´æ˜¯é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–åŠ é€Ÿåº“ï¼Œä¹Ÿç¡®å®æ˜¯å¦‚æ­¤ï¼Œæœ‰ç‚¹ç±»ä¼¼<line-code>llama.cpp</line-code>ã€‚å› ä¸ºç”µè„‘çš„é…ç½®æ¯”è¾ƒä½ï¼Œæ‰€ä»¥å¤§è¯­è¨€æ¨¡å‹åŠ é€Ÿç±»çš„é¡¹ç›®è¿˜æ˜¯å¾ˆæœ‰å¸å¼•åŠ›çš„ã€‚æ”¯æŒçš„æ¨¡å‹å¾ˆå¤šï¼Œä½¿ç”¨æ–¹å¼æ–¹å¼ä¹Ÿå¾ˆç®€å•ï¼Œæ‰€ä»¥è®¡åˆ’ç”¨æ­¤å¯¹ä¸åŒçš„å¤§æ¨¡å‹åšä»¥ä¸‹æµ‹è¯•ï¼šå†å²å¯¹è¯ã€çŸ¥è¯†åº“é—®ç­”ã€å°è£…APIç­‰</p>
            <p>è¿™æ¬¡çš„æµ‹è¯•ç¯å¢ƒè¿˜æ˜¯æˆ‘çš„è€æœ‹å‹ï¼Œå’Œåˆæ¬¡ä½“éªŒ<line-code>ChatGLM2-6B</line-code>ç”¨çš„æœºå™¨ç›¸åŒ</p>
            <h2>é¡¹ç›®ä»‹ç»</h2>
            <p>ä¸ºäº†æµ‹è¯•ä¸Šè¿°åŠŸèƒ½ï¼Œæ–°å»ºäº†ä¸€ä¸ªé¡¹ç›®ã€‚æ ¹æ®ä¸åŒçš„åŠŸèƒ½åšä¸€ä¸ªåˆ†ç±»ï¼š01_history_chat ã€02_knowledge_chat ã€ 03_api ã€ config ç­‰ã€‚</p>
            <h3>config ç›®å½•</h3>
            <p>è¿™é‡Œå­˜æ”¾ä¸€äº›é…ç½®æ–‡ä»¶æˆ–è€…å…¬å…±å˜é‡ç­‰</p>
            <h4>model.py</h4>
            <p>æ¨¡å‹ç›¸å…³çš„é…ç½®æ”¾åˆ°è¿™é‡Œï¼Œä¾‹å¦‚ï¼šæœ¬åœ°æ¨¡å‹çš„è·¯å¾„</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
model_path_dict = {
    "baichuan" : {
        "2-7B-Chat" : "D:\\llm\\baichuan-inc\\Baichuan2-7B-Chat",
    },
    "THUDM" : {
        "2-6b" : "D:\llm\THUDM\chatglm2-6b",
        "3-6b" : "D:\llm\THUDM\chatglm3-6b"
    },
    "Qwen" : {
        "7B-Chat" : "D:\llm\Qwen\Qwen-7B-Chat"
    },
    "01ai" : {
        "6B-Chat" : "D:\\llm\\01ai\\Yi-6B-Chat"
    }
}
            </pre>
            <h4>question.py</h4>
            <p>å‡†å¤‡äº†ä¸€äº›æµ‹è¯•é—®é¢˜ï¼Œæ”¾åœ¨äº†è¯¥æ–‡ä»¶ä¸­</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
history_chat_questions = ["ä½ å¥½", "ä¸­å›½çš„é¦–éƒ½æ˜¯", "ä»–çš„é¢ç§¯æ˜¯å¤šå°‘", "ä»–æœ‰å‡ åº§æœºåœº", "ä¸€å…±é—®äº†ä½ å‡ ä¸ªé—®é¢˜"]
            </pre>
            <h3>è™šæ‹Ÿç¯å¢ƒ</h3>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
#   åˆ›å»ºã€æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
python -m venv venv
.\venv\scripts\activate

#   ã€linuxã€‘åˆ›å»ºã€æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source ./venv/bin/activate

#   é€€å‡ºè™šæ‹Ÿç¯å¢ƒ
deactivate

pip install --pre --upgrade bigdl-llm[all]

#   ã€linuxã€‘å®‰è£…CPUç‰ˆæœ¬çš„ torch
#   è¯´æ˜ï¼šå¦‚æœéœ€è¦ linux ä¸‹ CPU ç‰ˆæœ¬çš„ torch ï¼Œè¯·å…ˆå®‰è£… torch ä¹‹åå†å®‰è£… bigdl-llm
#       ï¼ï¼ï¼æµ‹è¯•å‘ç°å…ˆå®‰è£… bigdl-llm ä¹‹åå†å®‰è£… CPU ç‰ˆæœ¬çš„ torch æ²¡æœ‰æˆåŠŸ
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
            </pre>
            <mark-block explain="è¯´æ˜">
                <p>bigdl-llm[all] åŒ…ä¾èµ– transformers ï¼Œ å®‰è£…æ—¶ä¼šè‡ªåŠ¨å°†å…¶å¸¦ä¸Š</p>
                <p>2024-02-26 bigdl-llm[all]çš„ç‰ˆæœ¬æ˜¯ï¼š2.5.0b20240225 ï¼Œå¯¹åº”çš„ transformers ç‰ˆæœ¬æ˜¯ï¼š4.31.0</p>
                <p>2024-02-27 bigdl-llm[all]çš„ç‰ˆæœ¬æ˜¯ï¼š2.5.0b20240226 ï¼Œå¯¹åº”çš„ transformers ç‰ˆæœ¬æ˜¯ï¼š4.31.0</p>
            </mark-block>
            <h2>å†å²å¯¹è¯</h2>
            <h3>Baichuan2-7B-Chat</h3>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
#   å¼•ç”¨æœ¬åœ°ä¾èµ–éœ€è¦ï¼Œä¾‹å¦‚ï¼šå¼•ç”¨ config.model
import sys
sys.path.append(".")

from transformers import AutoTokenizer
from bigdl.llm.transformers import AutoModelForCausalLM

from config.model import model_path_dict
from config.question import history_chat_questions

model_path = model_path_dict["baichuan"]["2-7B-Chat"]
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, load_in_4bit=True, trust_remote_code=True, use_cache=True)

#   model.chat æ–¹æ³•ä½äºæ¨¡å‹æƒé‡æ–‡ä»¶å¤¹çš„ modeling_baichuan.py æ–‡ä»¶ä¸­ï¼Œå¦‚ä¸‹ï¼š
'''
def chat(self, tokenizer, messages: List[dict], stream=False, generation_config: Optional[GenerationConfig]=None)
'''
#   è¯¥æ–¹æ³•æ²¡æœ‰ temperature ã€ top_p ç­‰å‚æ•°ï¼Œ generation_config å‚æ•°è¿™é‡Œæ²¡æœ‰ä¼ å…¥ã€‚é»˜è®¤é‡‡ç”¨çš„æ˜¯ generation_config.json æ–‡ä»¶ä¸­çš„é…ç½®
#   ä½ å¯ä»¥è¿™æ ·è·å–æ¨¡å‹æƒé‡çš„ GenerationConfig
'''
from transformers.generation.utils import GenerationConfig
generation_config = GenerationConfig.from_pretrained(model_path)
print(str(generation_config))
'''

messages=[]

for question in history_chat_questions:
    print("é—®ï¼š" + question)
    messages.append({"role": "user", "content": question})
    response = model.chat(tokenizer, messages)
    messages.append({"role": "assistant", "content": response})
    print("ç­”ï¼š" + str(response))

print(str(messages))
                </pre>
            </details>
            <p>è¿™ä¸ªæ¯”è¾ƒé¡ºåˆ©ï¼Œä»…å®‰è£…<line-code>bigdl-llm[all]</line-code>å³å¯ï¼Œä¸‹é¢çœ‹ä¸€ä¸‹è¿è¡Œæ•ˆæœï¼š</p>
            <p>
                <img src="./image/1.png" alt="bigdl-llm ä¹‹ Baichuan2-7B-Chat æ¨¡å‹ï¼Œå†å²å¯¹è¯æµ‹è¯•" />
            </p>
            <mark-block type="warning">
                <p>æ‰§è¡Œæ—¶ä¼šæœ‰å¦‚ä¸‹çš„è­¦å‘Š</p>
                <p>WARNING - Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers</p>
                <p>pip install xformers.</p>
            </mark-block>
            <h3>chatglm2-6b</h3>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
import sys
sys.path.append(".")

from transformers import AutoTokenizer
from bigdl.llm.transformers import AutoModel

from config.model import model_path_dict
from config.question import history_chat_questions

model_path = model_path_dict["THUDM"]["2-6b"]
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModel.from_pretrained(model_path, load_in_4bit=True, trust_remote_code=True)

#   model.chat æ–¹æ³•ä½äºæ¨¡å‹æƒé‡æ–‡ä»¶å¤¹çš„ modeling_chatglm.py æ–‡ä»¶ä¸­ï¼Œå¦‚ä¸‹ï¼š
'''
def chat(self, tokenizer, query: str, history: List[Tuple[str, str]] = None, max_length: int = 8192, num_beams=1,do_sample=True, top_p=0.8, temperature=0.8, logits_processor=None, **kwargs)
'''

messages = []
history = None

for question in history_chat_questions:
    print("é—®ï¼š" + question)
    response, history = model.chat(tokenizer, question, history=history)
    print("ç­”ï¼š" + str(response))

for question, response in history:
        messages.append({"role": "user", "content": question})
        messages.append({"role": "assistant", "content": response})
print(str(messages))
                </pre>
            </details>
            <p>chatglm2-6b æ¨¡å‹å®˜æ–¹å»ºè®®ä¾èµ–çš„ transformers ç‰ˆæœ¬ä¸ºï¼š4.30.2 ã€‚transformers 4.31.0ï¼Œè¿™ä¸ªç‰ˆæœ¬ä¹Ÿå¯ä»¥ï¼Œä¸‹é¢çœ‹ä¸€ä¸‹è¿è¡Œæ•ˆæœï¼š</p>
            <p>
                <img src="./image/2.png" alt="bigdl-llm ä¹‹ chatglm2-6b æ¨¡å‹ï¼Œå†å²å¯¹è¯æµ‹è¯•" />
            </p>
            <h3>chatglm3-6b</h3>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
import sys
sys.path.append(".")

from transformers import AutoTokenizer
from bigdl.llm.transformers import AutoModel

from config.model import model_path_dict
from config.question import history_chat_questions

model_path = model_path_dict["THUDM"]["3-6b"]
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModel.from_pretrained(model_path, load_in_4bit=True, trust_remote_code=True)

#   model.chat æ–¹æ³•ä½äºæ¨¡å‹æƒé‡æ–‡ä»¶å¤¹çš„ modeling_chatglm.py æ–‡ä»¶ä¸­ï¼Œå¦‚ä¸‹ï¼š
'''
def chat(self, tokenizer, query: str, history: List[Tuple[str, str]] = None, role: str = "user", max_length: int = 8192, num_beams=1, do_sample=True, top_p=0.8, temperature=0.8, logits_processor=None, **kwargs)
'''
#   ã€è¯´æ˜ã€‘æ„Ÿè§‰å½¢å‚ history çš„æ•°æ®ç±»å‹å†™é”™äº†ï¼Œè¯¥æ–¹æ³•å†…éƒ¨ä¼šè°ƒç”¨ tokenization_chatglm.py æ–‡ä»¶ä¸­çš„ build_chat_input æ–¹æ³•ï¼ˆç±»å‹æ˜æ˜¾ä¸ä¸€è‡´ï¼‰

history = None

for question in history_chat_questions:
    print("é—®ï¼š" + question)
    response, history = model.chat(tokenizer, question, history=history)
    print("ç­”ï¼š" + str(response))

print(str(history))
                </pre>
            </details>
            <p>chatglm3-6b ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œä»…å®‰è£…<line-code>bigdl-llm[all]</line-code>å³å¯ï¼Œä¸‹é¢çœ‹ä¸€ä¸‹è¿è¡Œæ•ˆæœï¼š</p>
            <p>
                <img src="./image/3.png" alt="bigdl-llm ä¹‹ chatglm3-6b æ¨¡å‹ï¼Œå†å²å¯¹è¯æµ‹è¯•" />
            </p>
            <h3>Qwen-7B-Chat</h3>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
import sys
sys.path.append(".")

from transformers import AutoTokenizer
from bigdl.llm.transformers import AutoModelForCausalLM

from config.model import model_path_dict
from config.question import history_chat_questions

model_path = model_path_dict["Qwen"]["7B-Chat"]
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, load_in_4bit=True, trust_remote_code=True)

#   model.chat æ–¹æ³•ä½äºæ¨¡å‹æƒé‡æ–‡ä»¶å¤¹çš„ modeling_qwen.py æ–‡ä»¶ä¸­ï¼Œå¦‚ä¸‹ï¼š
'''
def chat(self,tokenizer: PreTrainedTokenizer,query: str,history: Optional[HistoryType],system: str = "You are a helpful assistant.",append_history: bool = True,stream: Optional[bool] = _SENTINEL,stop_words_ids: Optional[List[List[int]]] = None,generation_config: Optional[GenerationConfig] = None,**kwargs,) -> Tuple[str, HistoryType]
'''    

messages=[]
history = None

for question in history_chat_questions:
    print("é—®ï¼š" + question)
    response, history = model.chat(tokenizer, question, history=history)
    print("ç­”ï¼š" + str(response))

for question, response in history:
        messages.append({"role": "user", "content": question})
        messages.append({"role": "assistant", "content": response})
print(str(messages))
                </pre>
            </details>
            <p>Qwen-7B-Chat ä¹Ÿè¿˜å¯ä»¥ï¼Œé™¤äº†å·²å®‰è£…çš„<line-code>bigdl-llm[all]</line-code>ï¼Œè¿˜éœ€è¦å®‰è£…<line-code>pip install tiktoken einops transformers_stream_generator</line-code>ï¼Œä¸‹é¢çœ‹ä¸€ä¸‹è¿è¡Œæ•ˆæœï¼š</p>
            <p>
                <img src="./image/4.png" alt="bigdl-llm ä¹‹ Qwen-7B-Chat æ¨¡å‹ï¼Œå†å²å¯¹è¯æµ‹è¯•" />
            </p>
            <h3>Yi-6B-Chat</h3>
            <p>æ¯”è¾ƒçˆ½çš„æ˜¯ï¼Œå‘ç°äº†è¿™ä¸ªç«™ç‚¹ï¼š<a href="https://replicate.com/01-ai/yi-6b-chat" target="_blank">01-ai/yi-6b-chat â€“ Run with an API on Replicate</a>ï¼Œé—æ†¾çš„æ˜¯æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„å…¶ä»–æ¨¡å‹ã€‚</p>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
import sys
sys.path.append(".")

from typing import Optional, Tuple, List
import copy

from transformers import AutoTokenizer
from bigdl.llm.transformers import AutoModelForCausalLM

from config.model import model_path_dict
from config.question import history_chat_questions

model_path = model_path_dict["01ai"]["6B-Chat"]
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, load_in_4bit=True, trust_remote_code=True)

#   è¿™ä¸ªæ¨¡å‹æ¯”è¾ƒè´¹åŠ²å•Šï¼Œæ²¡æœ‰ model.chat æ–¹æ³•ï¼Œä¸‹é¢æ˜¯è‡ªå·±å°è£…çš„ä¸€äº›æ–¹æ³•

def chat1(
    model,
    tokenizer,
    query: str,
    history: Optional[List[Tuple[str, str]]],
    system: str = "You are a helpful assistant.",
    **kwargs,
) -> Tuple[str, List[Tuple[str, str]]]:
    im_start, im_end = "&lt;|im_start|>", "&lt;|im_end|>"
    #   tokenizer.encode("&lt;|im_start|>", return_tensors="pt")   =>  6
    #   tokenizer.encode("&lt;|im_end|>", return_tensors="pt")     =>  7

    if history is None:
        history = []
    else:
        # make a copy of the user's input such that is is left untouched
        history = copy.deepcopy(history)

    raw_text = f"\n{im_start}system\n{system}{im_end}\n"

    for turn_query, turn_response in history:
        raw_text += f"{im_start}user\n{turn_query}{im_end}\n"
        raw_text += f"{im_start}assistant\n{turn_response}{im_end}\n"

    raw_text += f"{im_start}user\n{query}{im_end}\n"
    raw_text += f"{im_start}assistant\n"
    input_ids = tokenizer.encode(raw_text, return_tensors="pt")
    outputs = model.generate(input_ids,
                             do_sample=True,
                             max_new_tokens=4096,
                             top_p=0.8,
                             temperature=0.8)
    response = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)
    history.append((query, response))
    return response, history

def chat2(model, tokenizer, messages, system: str = "You are a helpful assistant."):
    im_start, im_end = "&lt;|im_start|>", "&lt;|im_end|>"

    prompt = f"\n{im_start}system\n{system}{im_end}\n"
    for message in messages:
        match message["role"]:
            case "user":
                prompt += f"{im_start}user\n{message['content']}{im_end}\n" 
            case "assistant":
                prompt += f"{im_start}assistant\n{message['content']}{im_end}\n"
    prompt += f"{im_start}assistant\n"
    input_ids = tokenizer.encode(prompt, return_tensors="pt")  
    outputs = model.generate(input_ids,
                             do_sample=True,
                             max_new_tokens=4096,
                             top_p=0.8,
                             temperature=0.8)
    response = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)
    return response

def chat3(model,tokenizer,messages):
    #   apply_chat_template æ–¹æ³•éœ€è¦å®‰è£…è¾ƒæ–°ç‰ˆæœ¬çš„ transformers
    input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
    output_ids = model.generate(input_ids)
    response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)
    return response

print("------ç¬¬ä¸€è½®------")
messages1=[]
history = None
for question in history_chat_questions:
    print("é—®ï¼š" + question)
    response, history = chat1(model, tokenizer, question, history=history)
    print("ç­”ï¼š" + str(response))
for question, response in history:
        messages1.append({"role": "user", "content": question})
        messages1.append({"role": "assistant", "content": response})
print(str(messages1))

print("------ç¬¬äºŒè½®------")
messages2=[]
for question in history_chat_questions:
    print("é—®ï¼š" + question)
    messages2.append({"role": "user", "content": question})
    response = chat2(model, tokenizer, messages2)
    messages2.append({"role": "assistant", "content": response})
    print("ç­”ï¼š" + str(response))
print(str(messages2))

print("------ç¬¬ä¸‰è½®------")
messages3=[]
for question in history_chat_questions:
    print("é—®ï¼š" + question)
    messages3.append({"role": "user", "content": question})
    response = chat3(model,tokenizer, messages3)
    messages3.append({"role": "assistant", "content": response})
    print("ç­”ï¼š" + str(response))
print(str(messages3))
                </pre>
            </details>
            <p>Yi-6B-Chat è¿™ä¸ªå°±æ¯”è¾ƒè´¹åŠ²å„¿äº†ï¼Œæ²¡æœ‰ç±»ä¼¼ä¸Šé¢çš„<line-code>model.chat</line-code>æ–¹æ³•ï¼Œå¾—è‡ªå·±å°è£…ã€‚å¯¹<line-code>transformers</line-code>çš„ç‰ˆæœ¬ä¹Ÿæ¯”è¾ƒæŒ‘å‰”ï¼Œå°è¯•äº†å¥½å‡ æ¬¡ï¼Œç›´åˆ°<line-code>4.35.0</line-code>è¿™ä¸ªç‰ˆæœ¬æ‰æˆåŠŸã€‚</p>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ transformers ç‰ˆæœ¬å°è¯•</summary>
                <mark-block type="error">
                    <dl>
                        <dt>bigdl-llm[all]ï¼š2.5.0b20240225 ï¼Œtransformersï¼š4.31.0</dt>
                        <dd>
                            <pre>
------ç¬¬ä¸€è½®------
é—®ï¼šä½ å¥½
ç­”ï¼šä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
é—®ï¼šä¸­å›½çš„é¦–éƒ½æ˜¯
ç­”ï¼šä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚
é—®ï¼šä»–çš„é¢ç§¯æ˜¯å¤šå°‘
ç­”ï¼šä»–çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿè¿™ä¸ªé—®é¢˜éœ€è¦æ›´å¤šçš„ä¿¡æ¯æ‰èƒ½ç»™å‡ºä¸€ä¸ªå‡†ç¡®çš„ç­”æ¡ˆã€‚å¦‚æœä½ èƒ½
æä¾›æ›´å¤šçš„å…³äºä»–çš„ä¿¡æ¯ï¼Œæ¯”å¦‚ä»–çš„åå­—ï¼Œå‡ºç”Ÿæ—¥æœŸï¼Œä½å€ç­‰ç­‰ï¼Œé‚£ä¹ˆæˆ‘å°±å¯ä»¥æ ¹æ® 
ä½ æ‰€æä¾›çš„è¿™äº›ä¿¡æ¯æ¥å¸®åŠ©ä½ å›ç­”è¿™ä¸ªé—®é¢˜ï¼šä»–çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ
é—®ï¼šä»–æœ‰å‡ åº§æœºåœº
ç­”ï¼šä»–æœ‰å‡ åº§æœºåœºï¼Ÿè¿™ä¸ªé—®é¢˜éœ€è¦æ›´å¤šçš„ä¿¡æ¯æ‰èƒ½ç»™å‡ºä¸€ä¸ªå‡†ç¡®çš„ç­”æ¡ˆã€‚å¦‚æœä½ èƒ½æ
ä¾›æ›´å¤šçš„å…³äºä»–çš„ä¿¡æ¯ï¼Œæ¯”å¦‚ä»–çš„åå­—ï¼Œå‡ºç”Ÿæ—¥æœŸï¼Œä½å€ç­‰ç­‰ï¼Œé‚£ä¹ˆæˆ‘å°±å¯ä»¥æ ¹æ®ä½  
æ‰€æä¾›çš„è¿™äº›ä¿¡æ¯æ¥å¸®åŠ©ä½ å›ç­”è¿™ä¸ªé—®é¢˜ï¼šä»–æœ‰å‡ åº§æœºåœºï¼Ÿ
é—®ï¼šä¸€å…±é—®äº†ä½ å‡ ä¸ªé—®é¢˜
ç­”ï¼šä¸€å…±é—®äº†ä½ å‡ ä¸ªé—®é¢˜ï¼Ÿè¿™ä¸ªé—®é¢˜éœ€è¦æ›´å¤šçš„ä¿¡æ¯æ‰èƒ½ç»™å‡ºä¸€ä¸ªå‡†ç¡®çš„ç­”æ¡ˆã€‚å¦‚æœ
ä½ èƒ½æä¾›æ›´å¤šçš„å…³äºä½ çš„ä¿¡æ¯ï¼Œæ¯”å¦‚ä½ çš„åå­—ï¼Œå‡ºç”Ÿæ—¥æœŸï¼Œä½å€ç­‰ç­‰ï¼Œé‚£ä¹ˆæˆ‘å°±å¯ä»¥ 
æ ¹æ®ä½ æ‰€æä¾›çš„è¿™äº›ä¿¡æ¯æ¥å¸®åŠ©ä½ å›ç­”è¿™ä¸ªé—®é¢˜ï¼šä¸€å…±é—®äº†ä½ å‡ ä¸ªé—®é¢˜ï¼Ÿï¼Ÿ
------ç¬¬äºŒè½®------
é—®ï¼šä½ å¥½
ç­”ï¼šä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
é—®ï¼šä¸­å›½çš„é¦–éƒ½æ˜¯
ç­”ï¼šä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚
é—®ï¼šä»–çš„é¢ç§¯æ˜¯å¤šå°‘
ç­”ï¼šä»–çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿçœ‹èµ·æ¥ä½ å¯èƒ½æ˜¯åœ¨è¯¢é—®æŸäººçš„é¢ç§¯ã€‚ä½†æ˜¯ï¼Œç”±äºç¼ºä¹ä¸Šä¸‹æ–‡ï¼Œ
æˆ‘æ— æ³•æä¾›ä½ è¯¢é—®çš„é¢ç§¯ã€‚å¦‚æœä½ èƒ½æä¾›æ›´å¤šçš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘å°†å¾ˆä¹æ„å¸®åŠ©ä½ æ‰¾åˆ°ä½ è¯¢ 
é—®çš„é¢ç§¯ã€‚
é—®ï¼šä»–æœ‰å‡ åº§æœºåœº
ç­”ï¼šä»–æœ‰å‡ åº§æœºåœºï¼Ÿçœ‹èµ·æ¥ä½ å¯èƒ½æ˜¯åœ¨è¯¢é—®æŸäººçš„æœºåœºæ•°é‡ã€‚ä½†æ˜¯ï¼Œç”±äºç¼ºä¹ä¸Šä¸‹æ–‡
ï¼Œæˆ‘æ— æ³•æä¾›ä½ è¯¢é—®çš„æœºåœºæ•°é‡ã€‚å¦‚æœä½ èƒ½æä¾›æ›´å¤šçš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘å°†å¾ˆä¹æ„å¸®åŠ©ä½ æ‰¾ 
åˆ°ä½ è¯¢é—®çš„æœºåœºæ•°é‡ã€‚
é—®ï¼šä¸€å…±é—®äº†ä½ å‡ ä¸ªé—®é¢˜
ç­”ï¼šä¸€å…±é—®äº†ä½ å‡ ä¸ªé—®é¢˜ï¼Ÿçœ‹èµ·æ¥ä½ å¯èƒ½æ˜¯åœ¨è¯¢é—®ä½ æ€»å…±è¢«é—®äº†å‡ ä¸ªé—®é¢˜ã€‚ä½†æ˜¯ï¼Œç”±
äºç¼ºä¹ä¸Šä¸‹æ–‡ï¼Œæˆ‘æ— æ³•æä¾›ä½ è¯¢é—®çš„ä¸Šä¸‹æ–‡ã€‚å¦‚æœä½ èƒ½æä¾›æ›´å¤šçš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘å°†å¾ˆä¹ 
æ„å¸®åŠ©ä½ æ‰¾åˆ°ä½ è¯¢é—®çš„ä¸Šä¸‹æ–‡ã€‚
------ç¬¬ä¸‰è½®------
é—®ï¼šä½ å¥½
Traceback (most recent call last):
  File "E:\llm\bigdl-001\01_history_chat\yi-6b-chat.py", line 109, in &lt;module>
    response = chat3(model,tokenizer, messages3)
  File "E:\llm\bigdl-001\01_history_chat\yi-6b-chat.py", line 77, in chat3 
    input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
AttributeError: 'LlamaTokenizerFast' object has no attribute 'apply_chat_template'
                            </pre>
                            <p><strong>è¯´æ˜ï¼š</strong>åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸‹æ²¡æœ‰æ ¹æ®ä¸Šä¸‹æ–‡ï¼ˆå¯¹è¯å†å²ï¼‰å›ç­”é—®é¢˜ï¼Œå¹¶ä¸”ç¬¬ä¸‰è½®è¿˜æŠ¥é”™äº†ï¼Œç¬¬ä¸‰è½®éœ€è¦æ›´é«˜ç‰ˆæœ¬çš„ transformers æ¥æ”¯æŒ apply_chat_template æ–¹æ³•</p>
                        </dd>
                        <dt>bigdl-llm[all]ï¼š2.5.0b20240225 ï¼Œtransformersï¼š4.38.1</dt>
                        <dd>
                            <p>è¿™ä¸ªç‰ˆæœ¬ä¼šæŠ¥å¦‚ä¸‹é”™è¯¯ï¼ˆå»æ‰äº†ä¸€äº›å †æ ˆä¿¡æ¯ï¼‰</p>
                            <p>TypeError: LlamaRotaryEmbedding.forward() missing 1 required positional argument: 'position_ids'</p>
                        </dd>
                        <dt>bigdl-llm[all]ï¼š2.5.0b20240225 ï¼Œtransformersï¼š4.37.0 æˆ–è€… 4.36.0</dt>
                        <dd>
                            <p>
                                <img src="./image/5.png" alt="bigdl-llm ä¹‹ Yi-6B-Chat æ¨¡å‹ï¼Œå†å²å¯¹è¯æµ‹è¯•ï¼Œtransformers==4.37.0 å¼‚å¸¸" />
                            </p>
                        </dd>
                        <dd>
                            <p>ä¹‹å‰æµ‹è¯•çš„æ—¶å€™ï¼ˆ2024-02-19ï¼‰ å®‰è£… transformers==4.37.2 é…åˆ bigdl æµ‹è¯•æ—¶ä¼šæœ‰å¦‚ä¸‹é”™è¯¯ï¼Œè¿™æ¬¡æ²¡æœ‰éªŒè¯ï¼Œå¦‚ä¸‹ï¼š</p>
                            <p>KeyError: 'Cache only has 0 layers, attempted to access layer with index 0'</p>
                        </dd>
                    </dl>
                </mark-block>
            </details>
            <p>åœ¨<line-code>transformers==4.35.0</line-code>è¿™ä¸ªç‰ˆæœ¬ä¸‹å†å²å¯¹è¯æ²¡æœ‰é—®é¢˜ï¼Œä¹Ÿèƒ½è§£å†³<line-code>apply_chat_template</line-code>çš„é—®é¢˜ï¼Œå¦‚ä¸‹å›¾ï¼š</p>
            <p>
                <img src="./image/6.png" alt="bigdl-llm ä¹‹ Yi-6B-Chat æ¨¡å‹ï¼Œå†å²å¯¹è¯æµ‹è¯•" />
            </p>
            <h2>çŸ¥è¯†åº“é—®ç­”</h2>
            <p>è¿™é‡Œçš„æµ‹è¯•æ˜¯åœ¨<line-code>Langchain-Chatchat</line-code>ä¹‹åï¼Œå‚è€ƒè¯¥é¡¹ç›®çš„å†™æ³•ï¼Œåšä¸€ä¸‹åŒ…å«å·²çŸ¥ä¿¡æ¯çš„å¯¹è¯æµ‹è¯•ã€‚</p>
            <h3>é¡¹ç›®è°ƒæ•´</h3>
            <p>æ–°å¢ç›®å½•<line-code>02_knowledge_chat</line-code>ï¼›é…ç½®ä¿®æ”¹ç­‰ã€‚</p>
            <p>1ã€<line-code>config</line-code>ç›®å½•ä¸‹æ–°å¢<line-code>prompt_template.py</line-code>ï¼Œä»£ç å¦‚ä¸‹ï¼š</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
#   å‚è€ƒ Langchain-Chatchat é¡¹ç›®çš„å†™æ³•
#   ä¼šåœ¨çŸ¥è¯†åº“é—®ç­”ä¸­ä½¿ç”¨
knowledge_chat_prompt_template="""&lt;æŒ‡ä»¤>æ ¹æ®å·²çŸ¥ä¿¡æ¯ï¼Œç®€æ´å’Œä¸“ä¸šçš„æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»ä¸­å¾—åˆ°ç­”æ¡ˆï¼Œè¯·è¯´ â€œæ ¹æ®å·²çŸ¥ä¿¡æ¯æ— æ³•å›ç­”è¯¥é—®é¢˜â€ï¼Œä¸å…è®¸åœ¨ç­”æ¡ˆä¸­æ·»åŠ ç¼–é€ æˆåˆ†ï¼Œç­”æ¡ˆè¯·ä½¿ç”¨ä¸­æ–‡ã€‚ &lt;/æŒ‡ä»¤>
&lt;å·²çŸ¥ä¿¡æ¯>{knowledge}&lt;/å·²çŸ¥ä¿¡æ¯>
&lt;é—®é¢˜>{question}&lt;/é—®é¢˜>"""
            </pre>
            <p>2ã€<line-code>config/question.py</line-code>æ–‡ä»¶ä¿®æ”¹ï¼Œå¦‚ä¸‹ï¼š</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
from .prompt_template import knowledge_chat_prompt_template

history_chat_questions = ["ä½ å¥½", "ä¸­å›½çš„é¦–éƒ½æ˜¯", "ä»–çš„é¢ç§¯æ˜¯å¤šå°‘", "ä»–æœ‰å‡ åº§æœºåœº", "ä¸€å…±é—®äº†ä½ å‡ ä¸ªé—®é¢˜"]

knowledge_chat_questions = [
    "ä½ å¥½",
    "ä¸­å›½çš„é¦–éƒ½æ˜¯",
    "ä»–çš„é¢ç§¯æ˜¯å¤šå°‘",
    "ä»–æœ‰å‡ åº§æœºåœº",
    "ä½ çŸ¥é“ xiaodu114 å—ï¼Ÿ",
    knowledge_chat_prompt_template.format(knowledge='''ä¸€ä¸ªå…³äºxiaodu114çš„ç§˜å¯†
xiaodu114å¾ˆå–œæ¬¢çƒç±»è¿åŠ¨ï¼Œä¾‹å¦‚ï¼šä¹’ä¹“çƒã€ç¾½æ¯›çƒã€ç½‘çƒç­‰ã€‚
xiaodu114çƒ­çˆ±å­¦ä¹ ï¼Œå¹¶ä¸”éå¸¸å–œæ¬¢ç¼–ç¨‹ï¼ŒğŸ¤­ğŸ¤­ğŸ¤­
ä½ çŸ¥é“å—ï¼Ÿxiaodu114è¿˜æ˜¯Redditçš„æ³¨å†Œç”¨æˆ·å“¦ï¼è®¿é—®åœ°å€ä¸ºï¼šhttps://www.reddit.com/user/xiaodu114/  
xiaodu114 ç®€ä»‹
xiaodu114æ˜¯GitHubçš„ä¸€ä¸ªç”¨æˆ·ï¼Œåœ¨GitHubä¸Šæœ‰6ä¸ªå¯ç”¨çš„å­˜å‚¨åº“ï¼Œåœ°å€ä¸ºï¼šhttps://github.com/xiaodu114  ã€‚
xiaodu114åŒæ—¶ä¹Ÿæ˜¯npmjsçš„æ³¨å†Œç”¨æˆ·ï¼Œç›®å‰å·²ç»åˆ›å»ºäº†ä¸¤ä¸ªåº“ï¼ša2bei4 å’Œ a2bei4-rollup-plugin ã€‚''', question="ä»‹ç»ä¸€ä¸‹ xiaodu114")]
            </pre>
            <h3>Baichuan2-7B-Chat</h3>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
import sys
sys.path.append(".")

from transformers import AutoTokenizer
from bigdl.llm.transformers import AutoModelForCausalLM

from config.model import model_path_dict
from config.question import knowledge_chat_questions

model_path = model_path_dict["baichuan"]["2-7B-Chat"]
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, load_in_4bit=True, trust_remote_code=True, use_cache=True)

def chat(model,tokenizer,messages:list[dict]):
    prompt = ""
    for message in messages:
        match message["role"]:
            case "user":
                prompt += "&lt;reserved_106>" + message["content"] + "\n"
            case "assistant":
                prompt += "&lt;reserved_107>" + message["content"] + "\n"
    prompt += "&lt;reserved_107>"    
    input_ids = tokenizer.encode(prompt, return_tensors="pt")  
     
    output_ids = model.generate(input_ids,
                                do_sample=True,
                                max_new_tokens=4096,
                                top_p=0.8,
                                temperature=0.8)
    response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)
    return response

messages=[]
for question in knowledge_chat_questions:
    print("é—®ï¼š" + question)
    messages.append({"role": "user", "content": question})
    response = chat(model, tokenizer, messages)
    messages.append({"role": "assistant", "content": response})
    print("ç­”ï¼š" + str(response))
print(str(messages))
                </pre>
            </details>
            <p>å®‰è£…çš„ä¾èµ–åŒ…å’Œä¸Šé¢å¯¹åº”çš„å†å²å¯¹è¯ç« èŠ‚ç›¸åŒã€‚ä¸‹é¢çœ‹ä¸€ä¸‹è¿è¡Œæ•ˆæœï¼š</p>
            <p>
                <img src="./image/7.png" alt="bigdl-llm ä¹‹ Baichuan2-7B-Chat æ¨¡å‹ï¼Œå·²æœ‰çŸ¥è¯†é—®ç­”æµ‹è¯•" />
            </p>
            <h3>chatglm2-6b</h3>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
import sys
sys.path.append(".")

from transformers import AutoTokenizer
from bigdl.llm.transformers import AutoModel

from config.model import model_path_dict
from config.question import knowledge_chat_questions

model_path = model_path_dict["THUDM"]["2-6b"]
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModel.from_pretrained(model_path, load_in_4bit=True, trust_remote_code=True)

messages=[]
history = None

for question in knowledge_chat_questions:
    print("é—®ï¼š" + question)
    response, history = model.chat(tokenizer, question, history=history)
    print("ç­”ï¼š" + str(response))

for question, response in history:
        messages.append({"role": "user", "content": question})
        messages.append({"role": "assistant", "content": response})
print(str(messages))
                </pre>
            </details>
            <p>å®‰è£…çš„ä¾èµ–åŒ…å’Œä¸Šé¢å¯¹åº”çš„å†å²å¯¹è¯ç« èŠ‚ç›¸åŒã€‚ä¸‹é¢çœ‹ä¸€ä¸‹è¿è¡Œæ•ˆæœï¼š</p>
            <p>
                <img src="./image/8.png" alt="bigdl-llm ä¹‹ chatglm2-6b æ¨¡å‹ï¼Œå·²æœ‰çŸ¥è¯†é—®ç­”æµ‹è¯•" />
            </p>
            <h3>chatglm3-6b</h3>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
import sys
sys.path.append(".")

from transformers import AutoTokenizer
from bigdl.llm.transformers import AutoModel

from config.model import model_path_dict
from config.question import knowledge_chat_questions

model_path = model_path_dict["THUDM"]["3-6b"]
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModel.from_pretrained(model_path, load_in_4bit=True, trust_remote_code=True)

history = None
for question in knowledge_chat_questions:
    print("é—®ï¼š" + question)
    response, history = model.chat(tokenizer, question, history=history)
    print("ç­”ï¼š" + str(response))

print(str(history))
                </pre>
            </details>
            <p>å®‰è£…çš„ä¾èµ–åŒ…å’Œä¸Šé¢å¯¹åº”çš„å†å²å¯¹è¯ç« èŠ‚ç›¸åŒã€‚ä¸‹é¢çœ‹ä¸€ä¸‹è¿è¡Œæ•ˆæœï¼š</p>
            <p>
                <img src="./image/9.png" alt="bigdl-llm ä¹‹ chatglm3-6b æ¨¡å‹ï¼Œå·²æœ‰çŸ¥è¯†é—®ç­”æµ‹è¯•" />
            </p>
            <h3>Qwen-7B-Chat</h3>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
import sys
sys.path.append(".")

from transformers import AutoTokenizer
from bigdl.llm.transformers import AutoModelForCausalLM

from config.model import model_path_dict
from config.question import knowledge_chat_questions

model_path = model_path_dict["Qwen"]["7B-Chat"]
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, load_in_4bit=True, trust_remote_code=True)

def chat(model, tokenizer, messages, system: str = "You are a helpful assistant."):
    im_start, im_end = "&lt;|im_start|>", "&lt;|im_end|>"

    prompt = f"{im_start}system\n{system}{im_end}\n"
    for message in messages:
        match message["role"]:
            case "user":
                prompt += f"{im_start}user\n{message['content']}{im_end}\n" 
            case "assistant":
                prompt += f"{im_start}assistant\n{message['content']}{im_end}\n"
    prompt += f"{im_start}assistant\n"
    input_ids = tokenizer.encode(prompt, return_tensors="pt")  
    outputs = model.generate(input_ids,
                             do_sample=True,
                             max_new_tokens=1024,
                             top_p=0.8,
                             temperature=0.8)
    response = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)
    return response

messages=[]
for question in knowledge_chat_questions:
    print("é—®ï¼š" + question)
    messages.append({"role": "user", "content": question})
    response = chat(model, tokenizer, messages)
    messages.append({"role": "assistant", "content": response})
    print("ç­”ï¼š" + str(response))
print(str(messages))
                </pre>
            </details>
            <p>å®‰è£…çš„ä¾èµ–åŒ…å’Œä¸Šé¢å¯¹åº”çš„å†å²å¯¹è¯ç« èŠ‚ç›¸åŒã€‚ä¸‹é¢çœ‹ä¸€ä¸‹è¿è¡Œæ•ˆæœï¼š</p>
            <p>
                <img src="./image/10.png" alt="bigdl-llm ä¹‹ Qwen-7B-Chat æ¨¡å‹ï¼Œå·²æœ‰çŸ¥è¯†é—®ç­”æµ‹è¯•" />
            </p>
            <h3>Yi-6B-Chat</h3>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
import sys
sys.path.append(".")

from transformers import AutoTokenizer
from bigdl.llm.transformers import AutoModelForCausalLM

from config.model import model_path_dict
from config.question import knowledge_chat_questions

model_path = model_path_dict["01ai"]["6B-Chat"]
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, load_in_4bit=True, trust_remote_code=True)

def chat3(model,tokenizer,messages):
    input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
    output_ids = model.generate(input_ids)
    response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)
    return response

messages3=[]
for question in knowledge_chat_questions:
    print("é—®ï¼š" + question)
    messages3.append({"role": "user", "content": question})
    response = chat3(model,tokenizer, messages3)
    messages3.append({"role": "assistant", "content": response})
    print("ç­”ï¼š" + str(response))
print(str(messages3))
                </pre>
            </details>
            <p>è¿™é‡Œé‡‡ç”¨çš„æ˜¯ä¸Šé¢å†å²å¯¹è¯ä¸­çš„â€œç¬¬ä¸‰è½®â€çš„æ–¹å¼ï¼Œå› æ­¤éœ€è¦<line-code>transformers==4.35.0</line-code></p>
            <p>å®‰è£…çš„ä¾èµ–åŒ…å’Œä¸Šé¢å¯¹åº”çš„å†å²å¯¹è¯ç« èŠ‚ç›¸åŒã€‚ä¸‹é¢çœ‹ä¸€ä¸‹è¿è¡Œæ•ˆæœï¼š</p>
            <p>
                <img src="./image/11.png" alt="bigdl-llm ä¹‹ Yi-6B-Chat æ¨¡å‹ï¼Œå·²æœ‰çŸ¥è¯†é—®ç­”æµ‹è¯•" />
            </p>
            <p>è¿™é‡Œå‡ºç°äº†ç‚¹é—®é¢˜ï¼Œå…ˆé—®ä¸€ä¸‹ï¼šâ€œä½ çŸ¥é“ xiaodu114 å—ï¼Ÿï¼ˆè¿™ä¸ªé—®é¢˜æ²¡æœ‰åŒ…å«å·²çŸ¥ä¿¡æ¯ï¼‰ï¼Œåé¢çš„é—®é¢˜æ˜¯ï¼šâ€œä»‹ç»ä¸€ä¸‹ xiaodu114â€ï¼ˆè¿™ä¸ªé—®é¢˜æ˜¯åŒ…å«å·²çŸ¥ä¿¡æ¯çš„ï¼‰ã€‚æˆªå›¾ä¸­ä½ å¯ä»¥çœ‹åˆ°ï¼Œæ²¡æœ‰æ ¹æ®å·²çŸ¥ä¿¡æ¯å›ç­”ï¼ŒçŒœæµ‹å¯èƒ½æ˜¯å—åˆ°ä¸Šä¸€ä¸ªé—®é¢˜çš„å½±å“ã€‚å†çœ‹ä¸€ä¸‹æ²¡æœ‰å¹²æ‰°é—®é¢˜çš„æƒ…å†µï¼Œå¦‚ä¸‹ï¼š</p>
            <p>
                <img src="./image/12.png" alt="bigdl-llm ä¹‹ Yi-6B-Chat æ¨¡å‹ï¼Œå·²æœ‰çŸ¥è¯†é—®ç­”æµ‹è¯•" />
            </p>
            <h3>å°ç»“</h3>
            <p>â€œä½ çŸ¥é“ xiaodu114 å—ï¼Ÿâ€ã€â€œä»‹ç»ä¸€ä¸‹ xiaodu114â€ï¼ˆåŒ…å«å·²çŸ¥ä¿¡æ¯ï¼‰ï¼Œè¿ç»­å¯¹è¯æ—¶æœ‰çš„å¤§æ¨¡å‹ä¼šå—åˆ°å†å²å¯¹è¯çš„å½±å“ã€‚å¦‚æœé—®çš„æ˜¯åŒä¸€ä¸ªé—®é¢˜ï¼šâ€œä½ çŸ¥é“ xiaodu114 å—ï¼Ÿâ€åªæ˜¯åé¢ä¸€ä¸ªåŒ…å«å·²çŸ¥ä¿¡æ¯ï¼Œè¿™ç§æƒ…å†µä¸‹æ›´å®¹æ˜“å—åˆ°å½±å“ï¼Œéƒ½ä¸å¤ªç¨³å®šï¼Œå¦‚ä¸‹ï¼š</p>
            <p>
                <img src="./image/13.png" alt="bigdl-llm ä¹‹ chatglm2-6b æ¨¡å‹ï¼Œå·²æœ‰çŸ¥è¯†é—®ç­”æµ‹è¯•" />
            </p>
            <h2>API</h2>
            <p>å‰é¢â€œå†å²å¯¹è¯â€å’Œâ€œçŸ¥è¯†é—®ç­”â€çš„æµ‹è¯•éƒ½æ˜¯ä¸ºäº†è¿™é‡Œçš„APIï¼Œå°†ä»–ä»¬è£…è¿›APIæ¥æ–¹ä¾¿ä½¿ç”¨ã€‚</p>
            <h3>Javascript å‰ç«¯</h3>
            <p>APIæä¾›ä¹‹åæ€ä¹ˆè°ƒç”¨ï¼Ÿä¸ºäº†æµ‹è¯•ä¸€ä¸‹è¿™äº›APIï¼Œå†™äº†ä¸€ä¸ªç®€å•çš„æµ‹è¯•é¡µé¢</p>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <div ddz-class="here-need-to-handle-by-highlight-and-request-html" data-url="./source/03-api-index.html" ddz-lang="html"></div>
            </details>
            <h3>é¡¹ç›®è°ƒæ•´</h3>
            <p>ä¸ºäº†æä¾›APIï¼Œè¿˜è¦å®‰è£…<line-code>pip install fastapi sse-starlette uvicorn</line-code>è¿™å‡ ä¸ªåº“ã€‚</p>
            <p>é¡¹ç›®æ›´ç›®å½•ä¸‹æ–°å¢<line-code>models</line-code>ç›®å½•ï¼Œæ”¾ç½®ä¸€äº›å®ä½“ç±»ã€‚åˆæ¬¡æ–°å¢<line-code>openai.py</line-code>ï¼Œä»£ç å¦‚ä¸‹ï¼š</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="python">
from typing import Literal, Optional, List, Dict, Any
from pydantic import BaseModel

class ChatMessage(BaseModel):
    role: Literal["user", "assistant", "system"]
    content: str

class ChatCompletionRequest(BaseModel):
    messages: List[ChatMessage]
    stream: Optional[bool] = False
    generation_config: Dict[str, Any] = {}
            </pre>
            <h3>Baichuan2-7B-Chat</h3>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <div ddz-class="here-need-to-handle-by-highlight-and-request-html" data-url="./source/03-api-baichuan2-7b-chat.txt" ddz-lang="python"></div>
            </details>
            <h3>chatglm2-6b</h3>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <div ddz-class="here-need-to-handle-by-highlight-and-request-html" data-url="./source/03-api-chatglm2-6b.txt" ddz-lang="python"></div>
            </details>
            <h3>chatglm3-6b</h3>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <div ddz-class="here-need-to-handle-by-highlight-and-request-html" data-url="./source/03-api-chatglm3-6b.txt" ddz-lang="python"></div>
            </details>
            <h3>Qwen-7B-Chat</h3>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <div ddz-class="here-need-to-handle-by-highlight-and-request-html" data-url="./source/03-api-qwen-7b-chat.txt" ddz-lang="python"></div>
            </details>
            <h3>Yi-6B-Chat</h3>
            <details>
                <summary>ç‚¹å‡»æŸ¥çœ‹ä»£ç </summary>
                <div ddz-class="here-need-to-handle-by-highlight-and-request-html" data-url="./source/03-api-yi-6b-chat.txt" ddz-lang="python"></div>
            </details>
            <p>è¿˜æ˜¯<line-code>Yi</line-code>æœ€éº»çƒ¦ï¼Œç›¸å¯¹äºå…¶ä»–å‡ ä¸ªã€‚ä¸Šé¢çš„ä»£ç å‚è€ƒ<line-code>Yi</line-code>GitHubé¡¹ç›®ä¸­çš„<line-code>demo\web_demo.py</line-code></p>
            <h3>å°ç»“</h3>
            <p>ä¸Šé¢çš„å‡ ä¸ªéƒ½åŒæ—¶æ”¯æŒæµå¼å“åº”å’Œéæµå¼å“åº”å¹¶ä¸”å‡ä»¥æµ‹è¯•é€šè¿‡ï¼Œåé¢æœ‰æœºä¼šåœ¨é€æ­¥å®Œå–„ã€‚</p>
        </div>
    </body>
</html>
