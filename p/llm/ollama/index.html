<!DOCTYPE html>
<html lang="zh-cmn-Hans">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta http-equiv="X-UA-Compatible" content="ie=edge" />
        <title>Ollama - xiaodu114.github.io</title>
        <meta name="keywords" content="Ollama,LLM,大语言模型,deepseek,deepseek-coder,Visual Studio Code,Privy,Code GPT: Chat & AI Agents,Continue - Claude, CodeLlama, GPT-4, and more" />
        <meta name="description" content="体验一下Ollama" />

        <script src="/p/_/js/main.js"></script>
        <script src="../_common/main.js"></script>
    </head>

    <body>
        <!-- github访问地址：/p/llm/ollama/index.html -->
        <div class="blog-page">
            <h1>Ollama</h1>
            <p>官网：<a href="https://ollama.ai/" target="_blank">Ollama</a></p>
            <p>GitHub：<a href="https://github.com/ollama/ollama" target="_blank">GitHub - ollama/ollama: Get up and running with Llama 2, Mistral, and other large language models locally.</a></p>
            <h2>安装</h2>
            <h3>Linux</h3>
            <p>这里的测试环境是：Ubuntu 22.04 + Intel(R)Xeon(R) CPU E5-2699 v4 2.20GHZ + RAM（512GB）</p>
            <h4>官网推荐</h4>
            <p>官网给的 Linux 的安装方式很简单，只需一个命令，如下：</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
curl https://ollama.ai/install.sh | sh
            </pre>
            <p>尝试了一下，安装失败了……看了一下原因，上面的脚本会在<line-code>GitHub</line-code>上下载文件，你懂的，被墙了，这台服务器又没有梯子……</p>
            <h4>自己动手</h4>
            <p>这点小事儿不能阻挡咱的脚步，是不？于是就看了一下上面的脚本，发现文件下载地址是这个：<a href="https://github.com/ollama/ollama/releases" target="_blank">Releases · ollama/ollama · GitHub</a>。于是就下了最新版本的（2024-02-01，版本是 0.1.22）：<line-code>ollama-linux-amd64</line-code></p>
            <p>问了一下大模型，下载了文件之后怎么安装，下面是他的回复：</p>
            <fieldset>
                <legend>大语言模型是这样回答的</legend>
                <p>如果已经提前下载了Ollama的Linux版本（ollama-linux-amd64文件），可以按照以下步骤进行安装：</p>
                <ol>
                    <li>将下载的ollama-linux-amd64文件复制到你希望安装Ollama的目录中。</li>
                    <li>打开终端，并进入到包含ollama-linux-amd64文件的目录。</li>
                    <li>运行以下命令，将ollama-linux-amd64文件设置为可执行文件： chmod +x ollama-linux-amd64</li>
                    <li>运行以下命令，将ollama-linux-amd64文件移动到/usr/local/bin目录（或其他你希望安装的目录）： sudo mv ollama-linux-amd64 /usr/local/bin/ollama</li>
                    <li>现在，你可以在终端中运行ollama命令来启动Ollama。</li>
                </ol>
                <p>注意：在上述步骤中，假设你已经拥有足够的权限来执行sudo命令。如果没有sudo权限，请使用其他适当的方式来移动ollama-linux-amd64文件到所需的目录。</p>
            </fieldset>
            <p>看了他的回复之后，瞬间明白了。这不就相当于 exe 嘛，不过这里是免安装的</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
sudo chmod +x ollama-linux-amd64
sudo mv ollama-linux-amd64 /usr/local/bin/ollama
            </pre>
            <p>
                <img src="./image/1.png" alt="安装 ollama-linux-amd64" />
            </p>
            <h2>跑起来</h2>
            <p>先看看他有哪些命令</p>
            <p>
                <img src="./image/2.png" alt="ollama -h" />
            </p>
            <p>看了这个<a href="https://ollama.ai/library/deepseek-coder" target="_blank">deepseek-coder</a>之后，选一个合适大小的大模型直接走起：</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
ollama run deepseek-coder:6.7b
            </pre>
            <p>
                <img src="./image/3.png" alt="执行命令： ollama run deepseek-coder:6.7b 异常" />
            </p>
            <h3>启动服务</h3>
            <p>按照上面的指示，需要先启动服务</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
#   默认监听 127.0.0.1
ollama serve
#   指定监听的IP地址
OLLAMA_HOST=192.168.x.x ollama serve
            </pre>
            <p>
                <img src="./image/4.png" alt="执行命令启动服务： ollama serve" />
            </p>
            <p>注意：上面的截图并不是第一次执行<line-code>ollama serve</line-code>；第一次启动时还会提示生成一个<line-code>Public Key</line-code>，暂时还没有用到这个key，如下图：</p>
            <p>
                <img src="./image/5.png" alt="第一次启动服务： ollama serve" />
            </p>
            <mark-block>
                <dl>
                    <dt><strong>2024-03-21</strong></dt>
                    <dd>&nbsp;</dd>
                    <dd style="line-height: 1.8">
                        再次回味时（要测试 Continue - Claude, CodeLlama, GPT-4, and more 插件），首先启动了<line-code>OLLAMA_HOST=192.168.x.x ollama serve</line-code>，之后运行一个模型<line-code>ollama run deepseek-coder:6.7b</line-code>，没想到提示服务没启动……后来换成<line-code>ollama serve</line-code>就好了。说明<line-code>ollama run</line-code>只能在<line-code>127.0.0.1</line-code>使用，这个命令做了两件事：如果模型没有拉取，则先拉取；之后进入命令行交互模式。也就是说<strong>已经拉取了某些模型之后，如果你只是想调用他的API，只需要<line-code>ollama serve</line-code>即可。</strong>后面测试发现，<line-code>ollama pull</line-code>也只能在<line-code>127.0.0.1</line-code>使用。
                    </dd>
                    <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
#   只能本地使用
ollama serve
ollama run llava:13b
ollama pull llava:13b

#   指定监听的IP地址
OLLAMA_HOST=192.168.x.x ollama serve
OLLAMA_HOST=192.168.x.x ollama run llava:13b
OLLAMA_HOST=192.168.x.x ollama pull llava:13b

#   总之，OLLAMA_HOST 这个要使用就都使用，要不都不使用
                    </pre>
                    <p>
                        <img src="./image/5-1.png" alt="OLLAMA_HOST=192.168.x.x ollama serve 指定IP地址并启动服务，ollama pull llava:13b 拉取模型同样指定IP地址" />
                    </p>
                </dl>
            </mark-block>
            <h3>拉取模型</h3>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
#   拉取一个模型
ollama pull deepseek-coder:6.7b

#   运行一个模型。如果本地没有，会先拉取。之后进入命令行交互模式
ollama run deepseek-coder:6.7b
            </pre>
            <p>
                <img src="./image/6.png" alt="ollama run deepseek-coder:6.7b  拉取模型部分" />
            </p>
            <p>
                <img src="./image/7.png" alt="ollama run deepseek-coder:6.7b  命令行交互部分" />
            </p>
            <mark-block>
                <dl>
                    <dt><strong>2024-04-08</strong></dt>
                    <dd style="line-height: 2">ollama 拉取的模型放到哪里了？</dd>
                    <dd>ubuntu 22.04 为例，路径为：<line-code>/home/xxx/.ollama/models/blobs</line-code></dd>
                    <dd>
                        <img src="./image/7-1.png" alt="ollama 缓存模型的位置" />
                    </dd>
                    <dd style="line-height: 2">上面是默认的缓存位置，你也可以动态设置缓存位置：<line-code>OLLAMA_MODELS=/home/xxx/llm/0-model/ollama/models OLLAMA_HOST=192.168.xxx.xxx ollama serve</line-code></dd>
                </dl>
            </mark-block>
            <h3>客户端</h3>
            <h4>curl</h4>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="bash">
curl http://localhost:11434/api/generate -d '{
  "model": "deepseek-coder:6.7b",
  "prompt":"javascript 实现斐波那契数列"
}'


curl http://localhost:11434/api/chat -d '{
  "model": "deepseek-coder:6.7b",
  "messages": [
    { "role": "user", "content": "javascript 实现斐波那契数列" }
  ]
}'
            </pre>
            <h4>Postman</h4>
            <p>
                <img src="./image/8.png" alt="Postman api/chat 测试" />
            </p>
            <p>你要看着有些费劲，你可以在请求参数中添加：<line-code>"stream": false</line-code>来禁用流式响应</p>
            <h3>Visual Studio Code</h3>
            <p>VS Code 算是客户端的一种，放在这里突出他的重要性……</p>
            <h4>Privy</h4>
            <p>市场位置：<a href="https://marketplace.visualstudio.com/items?itemName=privy.privy-vscode" target="_blank">Privy - AI coding assistant like GitHub Copilot that runs locally.</a></p>
            <p>这款插件支持的功能还挺多：自动补全|AI聊天|解释代码|生成测试|查找bug |诊断错误（说明：从上面的网址翻译的）。使用起来比较简单，下面看一下如何配置以及对话：</p>
            <p>
                <img src="./image/9.png" alt="Visual Studio Code 插件 Privy 配置以及AI对话" />
            </p>
            <p>选中你要搞的代码，看一下 Privy 具体能够干啥</p>
            <p>
                <img src="./image/10.png" alt="Privy 功能：自动补全|AI聊天|解释代码|生成测试|查找bug |诊断错误" />
            </p>
            <p>下面测试了一下【解释代码】功能，代码来自Javascript库<line-code>a2bei4</line-code>中的<line-code>MyId</line-code>。该类库已经上传至<a href="https://www.npmjs.com/package/a2bei4" target="_blank">a2bei4 - npm</a>；或者直接点击<a href="https://unpkg.com/a2bei4" target="_blank">unpkg.com/a2bei4</a>查看</p>
            <p>
                <img src="./image/11.png" alt="Privy 解释代码" />
            </p>
            <p>竟然是用英文回答的……他怎么知道咱的英文水平很溜，😅😅😅</p>
            <h4>Code GPT: Chat & AI Agents</h4>
            <p>官网：<a href="https://www.codegpt.co/" target="_blank">Code like a Pro with the CodeGPT AI Copilot</a> | <a href="https://docs.codegpt.co/zh-Hans/docs/intro" target="_blank">关于CodeGPT | CodeGPT</a></p>
            <p>这个插件感觉挺厉害，下载量不小，支持的 Provider 挺多的，当然这里测试的是<line-code>Ollama</line-code>。测试时发现，你只能在下拉框中选择 Provider ，选择之后没有配置IP地址和端口的地方，访问的是<line-code>localhost:11434</line-code>……我去，这是让我们人手一个的意思啊！之后查了一下，看看能不能配置IP地址，在 GitHub 的官方仓储上搜索的，有不少关于这个话题。其中的解决办法有：</p>
            <dl>
                <dt>选择 Provider 时，选择 Custom</dt>
                <dd>配置 Custom Link，这里配置的是： http://IP:11434/api/chat ; Key，随便写了一个</dd>
                <dd>对话时，从Ollama 服务打印的日志来看，访问的地址竟然是 http://IP:11434/chat/completions 。这不是 openai api 的格式吗？后来用 LM Studio 启动服务做测试，还真的可以</dd>
                <dt>用代理</dt>
                <dd>提到了<a href="https://github.com/BerriAI/litellm" target="_blank">LiteLLM</a>，这里没有测试</dd>
                <dd>倒是想到了正在使用的 nginx ，后面细说</dd>
            </dl>
            <p>凑巧正在尝试 nginx ，下面是<line-code>conf\nginx.conf</line-code>文件的配置内容：</p>
            <pre ddz-class="here-need-to-handle-by-highlight" ddz-lang="txt">
worker_processes  1;

events {
    worker_connections  1024;
}

http {
    include       mime.types;
    default_type  application/octet-stream;

    sendfile        on;

    keepalive_timeout  65;

    server {
        listen       11434;
        server_name  localhost;

        location / {
            #   访问 localhost:11434 时，会转发到下面的地址
            proxy_pass http://192.168.xxx.xxx:11434;
            proxy_connect_timeout 600s;
        } 
    }
}
            </pre>
            <p>配置好之后，选择 Provider 时就可以选择 Ollama 了，测试如下图：</p>
            <p>
                <img src="./image/12.png" alt="Code GPT: Chat & AI Agents 插件测试" />
            </p>
            <h4>Continue - Claude, CodeLlama, GPT-4, and more</h4>
            <p>插件地址：<a href="https://marketplace.visualstudio.com/items?itemName=Continue.continue" target="_blank">Continue - Claude, CodeLlama, GPT-4, and more - Visual Studio Marketplace</a></p>
            <p>GitHub官网：<a href="https://github.com/continuedev/continue" target="_blank">GitHub - continuedev/continue: ⏩ The easiest way to code with any LLM—Continue is an open-source autopilot for VS Code and JetBrains</a></p>
            <p>先占位置，知道有这么个插件，先简单的说一下该插件怎么使用</p>
            <h5>如何配置</h5>
            <p>按照下图中的步骤操作即可</p>
            <p>
                <img src="./image/13.png" alt="Continue - Claude, CodeLlama, GPT-4, and more 插件配置过程" />
            </p>
            <h5>对话测试</h5>
            <p>
                <img src="./image/14.png" alt="Continue - Claude, CodeLlama, GPT-4, and more 插件对话测试" />
            </p>
        </div>
    </body>
</html>
